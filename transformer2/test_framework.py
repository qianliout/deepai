"""
Transformer2æ¡†æ¶æµ‹è¯•è„šæœ¬
éªŒè¯æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import torch
import logging
from pathlib import Path

from config import TRANSFORMER_CONFIG, TRAINING_CONFIG, DATA_CONFIG, print_config, update_config_for_quick_test, setup_logging
from model import Transformer
from transformer import (
    PositionalEncoding,
    MultiHeadAttention,
    FeedForward,
    EncoderLayer,
    DecoderLayer,
    create_look_ahead_mask,
    create_combined_mask,
)
from data_loader import SimpleTokenizer, DataManager
from trainer import Trainer
from inference import TransformerInference

logger = logging.getLogger("Transformer2")


def test_config():
    """æµ‹è¯•é…ç½®ç³»ç»Ÿ"""
    print("ğŸ§ª æµ‹è¯•é…ç½®ç³»ç»Ÿ...")

    # æµ‹è¯•é…ç½®æ‰“å°
    print_config()

    # æµ‹è¯•é…ç½®å€¼æ˜¯å¦åˆç†
    assert TRANSFORMER_CONFIG.d_model > 0, "d_modelå¿…é¡»å¤§äº0"
    assert TRANSFORMER_CONFIG.n_heads > 0, "n_headså¿…é¡»å¤§äº0"
    assert TRANSFORMER_CONFIG.d_model % TRANSFORMER_CONFIG.n_heads == 0, "d_modelå¿…é¡»èƒ½è¢«n_headsæ•´é™¤"
    assert TRAINING_CONFIG.batch_size > 0, "batch_sizeå¿…é¡»å¤§äº0"
    assert DATA_CONFIG.max_length > 0, "max_lengthå¿…é¡»å¤§äº0"

    print("âœ… é…ç½®ç³»ç»Ÿæµ‹è¯•é€šè¿‡")


def test_transformer_components():
    """æµ‹è¯•Transformeræ ¸å¿ƒç»„ä»¶"""
    print("\nğŸ§ª æµ‹è¯•Transformeræ ¸å¿ƒç»„ä»¶...")

    batch_size, seq_len, d_model = 2, 10, 256
    n_heads = 4

    # æµ‹è¯•ä½ç½®ç¼–ç 
    pos_encoding = PositionalEncoding(d_model, 512)
    x = torch.randn(batch_size, seq_len, d_model)
    pos_out = pos_encoding(x)
    assert pos_out.shape == (batch_size, seq_len, d_model), f"ä½ç½®ç¼–ç è¾“å‡ºshapeé”™è¯¯: {pos_out.shape}"

    # æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›
    attention = MultiHeadAttention(d_model, n_heads)
    query = key = value = torch.randn(batch_size, seq_len, d_model)
    attn_out, attn_weights = attention(query, key, value)
    assert attn_out.shape == (batch_size, seq_len, d_model), f"æ³¨æ„åŠ›è¾“å‡ºshapeé”™è¯¯: {attn_out.shape}"
    assert attn_weights.shape == (batch_size, n_heads, seq_len, seq_len), f"æ³¨æ„åŠ›æƒé‡shapeé”™è¯¯: {attn_weights.shape}"

    # æµ‹è¯•å‰é¦ˆç½‘ç»œ
    ff = FeedForward(d_model, d_model * 4)
    ff_out = ff(x)
    assert ff_out.shape == (batch_size, seq_len, d_model), f"å‰é¦ˆç½‘ç»œè¾“å‡ºshapeé”™è¯¯: {ff_out.shape}"

    # æµ‹è¯•ç¼–ç å™¨å±‚
    encoder_layer = EncoderLayer(d_model, n_heads, d_model * 4)
    enc_out = encoder_layer(x)
    assert enc_out.shape == (batch_size, seq_len, d_model), f"ç¼–ç å™¨å±‚è¾“å‡ºshapeé”™è¯¯: {enc_out.shape}"

    # æµ‹è¯•è§£ç å™¨å±‚
    decoder_layer = DecoderLayer(d_model, n_heads, d_model * 4)
    dec_out = decoder_layer(x, x)  # ä½¿ç”¨xä½œä¸ºç¼–ç å™¨è¾“å‡º
    assert dec_out.shape == (batch_size, seq_len, d_model), f"è§£ç å™¨å±‚è¾“å‡ºshapeé”™è¯¯: {dec_out.shape}"

    print("âœ… Transformeræ ¸å¿ƒç»„ä»¶æµ‹è¯•é€šè¿‡")


def test_model():
    """æµ‹è¯•å®Œæ•´æ¨¡å‹"""
    print("\nğŸ§ª æµ‹è¯•å®Œæ•´æ¨¡å‹...")

    # åˆ›å»ºæ¨¡å‹
    model = Transformer()

    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size = 2
    src_seq_len = 10
    tgt_seq_len = 8

    src = torch.randint(0, TRANSFORMER_CONFIG.vocab_size_src, (batch_size, src_seq_len))
    tgt = torch.randint(0, TRANSFORMER_CONFIG.vocab_size_tgt, (batch_size, tgt_seq_len))

    # åˆ›å»ºæ©ç 
    tgt_mask = create_combined_mask(tgt)

    # å‰å‘ä¼ æ’­
    logits = model(src, tgt, None, tgt_mask)
    expected_shape = (batch_size, tgt_seq_len, TRANSFORMER_CONFIG.vocab_size_tgt)
    assert logits.shape == expected_shape, f"æ¨¡å‹è¾“å‡ºshapeé”™è¯¯: {logits.shape}, æœŸæœ›: {expected_shape}"

    # æµ‹è¯•ç¼–ç å™¨å•ç‹¬ä½¿ç”¨
    encoder_out = model.encode(src)
    expected_enc_shape = (batch_size, src_seq_len, TRANSFORMER_CONFIG.d_model)
    assert encoder_out.shape == expected_enc_shape, f"ç¼–ç å™¨è¾“å‡ºshapeé”™è¯¯: {encoder_out.shape}"

    print("âœ… å®Œæ•´æ¨¡å‹æµ‹è¯•é€šè¿‡")


def test_tokenizer():
    """æµ‹è¯•åˆ†è¯å™¨"""
    print("\nğŸ§ª æµ‹è¯•åˆ†è¯å™¨...")

    tokenizer = SimpleTokenizer()

    # æ„å»ºç®€å•è¯æ±‡è¡¨
    src_texts = ["hello world", "how are you", "good morning"]
    tgt_texts = ["ciao mondo", "come stai", "buongiorno"]
    tokenizer.build_vocab(src_texts, tgt_texts)

    # æµ‹è¯•ç¼–ç 
    text = "hello world"
    ids = tokenizer.encode(text, "src", 10)
    assert len(ids) == 10, f"ç¼–ç é•¿åº¦é”™è¯¯: {len(ids)}"

    # æµ‹è¯•è§£ç 
    decoded = tokenizer.decode(ids, "src")
    assert isinstance(decoded, str), "è§£ç ç»“æœä¸æ˜¯å­—ç¬¦ä¸²"

    # æµ‹è¯•ä¸å¡«å……æ¨¡å¼
    ids_no_pad = tokenizer.encode(text, "src", 10, pad_to_max=False)
    assert len(ids_no_pad) <= 10, f"ä¸å¡«å……æ¨¡å¼é•¿åº¦é”™è¯¯: {len(ids_no_pad)}"

    print("âœ… åˆ†è¯å™¨æµ‹è¯•é€šè¿‡")


def test_data_manager():
    """æµ‹è¯•æ•°æ®ç®¡ç†å™¨ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    print("\nğŸ§ª æµ‹è¯•æ•°æ®ç®¡ç†å™¨...")

    # ç”±äºæ•°æ®ä¸‹è½½å¯èƒ½å¾ˆæ…¢ï¼Œè¿™é‡Œåªæµ‹è¯•åˆå§‹åŒ–
    data_manager = DataManager()
    assert data_manager.tokenizer is not None, "æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥"

    print("âœ… æ•°æ®ç®¡ç†å™¨æµ‹è¯•é€šè¿‡")


def test_inference():
    """æµ‹è¯•æ¨ç†åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•æ¨ç†åŠŸèƒ½...")

    # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹
    model_path = Path("./transformer2_quick_test/best_model.pt")
    if not model_path.exists():
        print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè·³è¿‡æ¨ç†æµ‹è¯•")
        return

    try:
        # åˆ›å»ºæ¨ç†å™¨
        inference = TransformerInference(str(model_path))

        # æµ‹è¯•ç¿»è¯‘
        result = inference.translate("hello", max_length=10)
        assert isinstance(result, str), "ç¿»è¯‘ç»“æœä¸æ˜¯å­—ç¬¦ä¸²"

        print("âœ… æ¨ç†åŠŸèƒ½æµ‹è¯•é€šè¿‡")
    except Exception as e:
        print(f"âš ï¸  æ¨ç†æµ‹è¯•å¤±è´¥: {e}")


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹Transformer2æ¡†æ¶æµ‹è¯•")
    print("=" * 60)

    # è®¾ç½®æ—¥å¿—
    setup_logging()

    # åˆ‡æ¢åˆ°å¿«é€Ÿæµ‹è¯•æ¨¡å¼
    update_config_for_quick_test()

    try:
        # è¿è¡Œå„é¡¹æµ‹è¯•
        test_config()
        test_transformer_components()
        test_model()
        test_tokenizer()
        test_data_manager()
        test_inference()

        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Transformer2æ¡†æ¶å·¥ä½œæ­£å¸¸")
        print("=" * 60)

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    run_all_tests()
