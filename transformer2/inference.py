"""
æ¨ç†æ¨¡å— - é‡æ„ç‰ˆæœ¬
è´Ÿè´£æ¨¡å‹æ¨ç†å’Œæ–‡æœ¬ç¿»è¯‘
è¯¦ç»†çš„æ•°æ®æµè½¬æ³¨é‡Šå’Œshapeè¯´æ˜
"""

import torch
import torch.nn.functional as F
import logging
import os
from typing import List, Optional

from config import TRANSFORMER_CONFIG, TRAINING_CONFIG
from model import Transformer
from data_loader import SimpleTokenizer
from transformer import create_padding_mask, create_look_ahead_mask

logger = logging.getLogger("Transformer2")


class TransformerInference:
    """Transformeræ¨ç†å™¨

    è´Ÿè´£åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å¹¶è¿›è¡Œæ–‡æœ¬ç¿»è¯‘

    æ•°æ®æµè½¬ï¼š
    è¾“å…¥æ–‡æœ¬ -> åˆ†è¯ç¼–ç  -> ç¼–ç å™¨ -> è§£ç å™¨(é€æ­¥ç”Ÿæˆ) -> è§£ç è¾“å‡ºæ–‡æœ¬
    """

    def __init__(self, model_path: str, vocab_dir: Optional[str] = None):
        """
        åˆå§‹åŒ–æ¨ç†å™¨

        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            vocab_dir: è¯æ±‡è¡¨ç›®å½•è·¯å¾„
        """
        self.device = TRAINING_CONFIG.device
        self.model_path = model_path

        # åŠ è½½æ¨¡å‹
        self.model = self._load_model()

        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = self._load_tokenizer(vocab_dir)

        logger.info("æ¨ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    def _load_model(self) -> Transformer:
        """
        åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹

        Returns:
            åŠ è½½çš„Transformeræ¨¡å‹
        """
        logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_path}")

        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}")

        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(self.model_path, map_location=self.device)

        # æ›´æ–°é…ç½®(å¦‚æœæ£€æŸ¥ç‚¹ä¸­åŒ…å«é…ç½®)
        if "config" in checkpoint:
            config_dict = checkpoint["config"]["transformer"]
            for key, value in config_dict.items():
                if hasattr(TRANSFORMER_CONFIG, key):
                    setattr(TRANSFORMER_CONFIG, key, value)

        # åˆ›å»ºæ¨¡å‹
        model = Transformer()
        model.load_state_dict(checkpoint["model_state_dict"])
        model.to(self.device)
        model.eval()

        logger.info("æ¨¡å‹åŠ è½½å®Œæˆ")
        return model

    def _load_tokenizer(self, vocab_dir: Optional[str] = None) -> SimpleTokenizer:
        """
        åŠ è½½åˆ†è¯å™¨

        Args:
            vocab_dir: è¯æ±‡è¡¨ç›®å½•è·¯å¾„

        Returns:
            åŠ è½½çš„åˆ†è¯å™¨
        """
        if vocab_dir is None:
            vocab_dir = TRAINING_CONFIG.vocab_save_dir

        logger.info(f"æ­£åœ¨åŠ è½½åˆ†è¯å™¨: {vocab_dir}")

        tokenizer = SimpleTokenizer()

        if os.path.exists(vocab_dir):
            tokenizer.load_vocab(vocab_dir)
        else:
            logger.warning(f"è¯æ±‡è¡¨ç›®å½•ä¸å­˜åœ¨: {vocab_dir}")
            raise FileNotFoundError(f"è¯æ±‡è¡¨ç›®å½•ä¸å­˜åœ¨: {vocab_dir}")

        logger.info("åˆ†è¯å™¨åŠ è½½å®Œæˆ")
        return tokenizer

    def translate(
        self,
        src_text: str,
        max_length: int = 100,
        beam_size: int = 1,
        temperature: float = 1.0,
    ) -> str:
        """
        ç¿»è¯‘æ–‡æœ¬

        Args:
            src_text: æºæ–‡æœ¬
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            beam_size: beam searchå¤§å° (1è¡¨ç¤ºè´ªå¿ƒæœç´¢)
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§

        Returns:
            ç¿»è¯‘åçš„æ–‡æœ¬

        æ•°æ®æµè½¬ï¼š
        æºæ–‡æœ¬ -> åˆ†è¯ç¼–ç  -> [batch_size=1, src_seq_len] ->
        ç¼–ç å™¨ -> [1, src_seq_len, d_model] ->
        è§£ç å™¨é€æ­¥ç”Ÿæˆ -> [1, tgt_seq_len] -> è§£ç ä¸ºæ–‡æœ¬
        """
        logger.info(f"å¼€å§‹ç¿»è¯‘: {src_text}")

        with torch.no_grad():
            if beam_size == 1:
                result = self._greedy_decode(src_text, max_length, temperature)
            else:
                result = self._beam_search_decode(src_text, max_length, beam_size, temperature)

        logger.info(f"ç¿»è¯‘ç»“æœ: {result}")
        return result

    def _greedy_decode(self, src_text: str, max_length: int, temperature: float) -> str:
        """
        è´ªå¿ƒè§£ç 

        Args:
            src_text: æºæ–‡æœ¬
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: æ¸©åº¦å‚æ•°

        Returns:
            ç¿»è¯‘ç»“æœ
        """
        # 1. ç¼–ç æºåºåˆ—
        # ä½¿ç”¨è®­ç»ƒæ—¶çš„æœ€å¤§åºåˆ—é•¿åº¦ï¼Œä½†ä¸å¡«å……åˆ°æœ€å¤§é•¿åº¦ï¼Œé¿å…åœ¨æ¨ç†æ—¶è¶…é•¿
        max_seq_len = TRANSFORMER_CONFIG.max_seq_len
        src_ids = self.tokenizer.encode(src_text, "src", max_seq_len, pad_to_max=False)
        src_tensor = torch.tensor([src_ids], device=self.device)  # [1, actual_src_len]

        # 2. ç¼–ç å™¨å‰å‘ä¼ æ’­
        encoder_output = self.model.encode(src_tensor, None)  # [1, src_seq_len, d_model]

        # 3. åˆå§‹åŒ–è§£ç å™¨è¾“å…¥
        bos_id = self.tokenizer.vocab_tgt[self.tokenizer.bos_token]
        eos_id = self.tokenizer.vocab_tgt[self.tokenizer.eos_token]

        # è§£ç å™¨è¾“å…¥ä»BOS tokenå¼€å§‹
        tgt_ids = [bos_id]

        # 4. é€æ­¥ç”Ÿæˆ
        for step in range(max_length):
            # æ£€æŸ¥åºåˆ—é•¿åº¦æ˜¯å¦ä¼šè¶…è¿‡æ¨¡å‹é™åˆ¶
            if len(tgt_ids) >= TRANSFORMER_CONFIG.max_seq_len:
                logger.warning(f"è¾¾åˆ°æœ€å¤§åºåˆ—é•¿åº¦é™åˆ¶ {TRANSFORMER_CONFIG.max_seq_len}ï¼Œåœæ­¢ç”Ÿæˆ")
                break

            # å½“å‰ç›®æ ‡åºåˆ—
            tgt_tensor = torch.tensor([tgt_ids], device=self.device)  # [1, current_len]

            # åˆ›å»ºç›®æ ‡åºåˆ—æ©ç 
            tgt_len = tgt_tensor.size(1)
            tgt_mask = create_look_ahead_mask(tgt_len, self.device)  # [current_len, current_len]

            # è§£ç å™¨å‰å‘ä¼ æ’­
            logits = self.model.decode_step(tgt_tensor, encoder_output, tgt_mask, None)  # [1, current_len, vocab_size]

            # è·å–æœ€åä¸€ä¸ªä½ç½®çš„logits
            next_token_logits = logits[0, -1, :] / temperature  # [vocab_size]

            # é€‰æ‹©ä¸‹ä¸€ä¸ªtoken
            if temperature == 1.0:
                next_token_id = torch.argmax(next_token_logits).item()
            else:
                probs = F.softmax(next_token_logits, dim=-1)
                next_token_id = torch.multinomial(probs, 1).item()

            # æ·»åŠ åˆ°åºåˆ—
            tgt_ids.append(next_token_id)

            # å¦‚æœç”Ÿæˆäº†EOS tokenï¼Œåœæ­¢ç”Ÿæˆ
            if next_token_id == eos_id:
                break

            logger.debug(f"ç”Ÿæˆæ­¥éª¤ {step+1}: token_id={next_token_id}")

        # 5. è§£ç ä¸ºæ–‡æœ¬
        result = self.tokenizer.decode(tgt_ids, "tgt")
        return result

    def _beam_search_decode(self, src_text: str, max_length: int, beam_size: int, temperature: float) -> str:
        """
        Beam searchè§£ç 

        Args:
            src_text: æºæ–‡æœ¬
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            beam_size: beamå¤§å°
            temperature: æ¸©åº¦å‚æ•°

        Returns:
            ç¿»è¯‘ç»“æœ
        """
        # 1. ç¼–ç æºåºåˆ—
        # ä½¿ç”¨è®­ç»ƒæ—¶çš„æœ€å¤§åºåˆ—é•¿åº¦ï¼Œä½†ä¸å¡«å……åˆ°æœ€å¤§é•¿åº¦ï¼Œé¿å…åœ¨æ¨ç†æ—¶è¶…é•¿
        max_seq_len = TRANSFORMER_CONFIG.max_seq_len
        src_ids = self.tokenizer.encode(src_text, "src", max_seq_len, pad_to_max=False)
        src_tensor = torch.tensor([src_ids], device=self.device)  # [1, actual_src_len]

        # 2. ç¼–ç å™¨å‰å‘ä¼ æ’­
        encoder_output = self.model.encode(src_tensor, None)  # [1, src_seq_len, d_model]

        # 3. åˆå§‹åŒ–beam
        bos_id = self.tokenizer.vocab_tgt[self.tokenizer.bos_token]
        eos_id = self.tokenizer.vocab_tgt[self.tokenizer.eos_token]

        # beamä¸­çš„å€™é€‰åºåˆ—: (åºåˆ—, ç´¯ç§¯åˆ†æ•°)
        beams = [([bos_id], 0.0)]
        completed_sequences = []

        # 4. Beam search
        for step in range(max_length):
            candidates = []

            for seq, score in beams:
                # å¦‚æœåºåˆ—å·²ç»ç»“æŸï¼Œç›´æ¥æ·»åŠ åˆ°å€™é€‰ä¸­
                if seq[-1] == eos_id:
                    candidates.append((seq, score))
                    continue

                # å½“å‰ç›®æ ‡åºåˆ—
                tgt_tensor = torch.tensor([seq], device=self.device)  # [1, current_len]

                # åˆ›å»ºç›®æ ‡åºåˆ—æ©ç 
                tgt_len = tgt_tensor.size(1)
                tgt_mask = create_look_ahead_mask(tgt_len, self.device)

                # è§£ç å™¨å‰å‘ä¼ æ’­
                logits = self.model.decode_step(tgt_tensor, encoder_output, tgt_mask, None)  # [1, current_len, vocab_size]

                # è·å–æœ€åä¸€ä¸ªä½ç½®çš„logits
                next_token_logits = logits[0, -1, :] / temperature  # [vocab_size]
                log_probs = F.log_softmax(next_token_logits, dim=-1)

                # é€‰æ‹©top-kä¸ªå€™é€‰
                top_k_probs, top_k_ids = torch.topk(log_probs, beam_size)

                for i in range(beam_size):
                    new_seq = seq + [top_k_ids[i].item()]
                    new_score = score + top_k_probs[i].item()
                    candidates.append((new_seq, new_score))

            # é€‰æ‹©æœ€ä½³çš„beam_sizeä¸ªå€™é€‰
            candidates.sort(key=lambda x: x[1], reverse=True)
            beams = candidates[:beam_size]

            # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæˆçš„åºåˆ—
            for seq, score in beams:
                if seq[-1] == eos_id and (seq, score) not in completed_sequences:
                    completed_sequences.append((seq, score))

            # å¦‚æœæœ‰è¶³å¤Ÿçš„å®Œæˆåºåˆ—ï¼Œå¯ä»¥æå‰åœæ­¢
            if len(completed_sequences) >= beam_size:
                break

        # 5. é€‰æ‹©æœ€ä½³åºåˆ—
        if completed_sequences:
            best_seq, _ = max(completed_sequences, key=lambda x: x[1])
        else:
            best_seq, _ = max(beams, key=lambda x: x[1])

        # 6. è§£ç ä¸ºæ–‡æœ¬
        result = self.tokenizer.decode(best_seq, "tgt")
        return result

    def translate_batch(self, src_texts: List[str], **kwargs) -> List[str]:
        """
        æ‰¹é‡ç¿»è¯‘

        Args:
            src_texts: æºæ–‡æœ¬åˆ—è¡¨
            **kwargs: ä¼ é€’ç»™translateæ–¹æ³•çš„å‚æ•°

        Returns:
            ç¿»è¯‘ç»“æœåˆ—è¡¨
        """
        results = []
        for src_text in src_texts:
            result = self.translate(src_text, **kwargs)
            results.append(result)
        return results


def interactive_translation(model_path: str, vocab_dir: Optional[str] = None):
    """
    äº¤äº’å¼ç¿»è¯‘

    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        vocab_dir: è¯æ±‡è¡¨ç›®å½•è·¯å¾„
    """
    print("ğŸš€ å¯åŠ¨äº¤äº’å¼ç¿»è¯‘å™¨...")

    try:
        # åˆå§‹åŒ–æ¨ç†å™¨
        inference = TransformerInference(model_path, vocab_dir)

        print("âœ… æ¨ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print("ğŸ’¡ è¾“å…¥ 'quit' é€€å‡ºç¨‹åº")
        print("ğŸ’¡ è¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©")
        print("-" * 50)

        while True:
            try:
                # è·å–ç”¨æˆ·è¾“å…¥
                src_text = input("\nè¯·è¾“å…¥è¦ç¿»è¯‘çš„æ–‡æœ¬: ").strip()

                if src_text.lower() == "quit":
                    print("ğŸ‘‹ å†è§!")
                    break
                elif src_text.lower() == "help":
                    print("å¸®åŠ©ä¿¡æ¯:")
                    print("  - ç›´æ¥è¾“å…¥æ–‡æœ¬è¿›è¡Œç¿»è¯‘")
                    print("  - è¾“å…¥ 'quit' é€€å‡ºç¨‹åº")
                    print("  - è¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©")
                    continue
                elif not src_text:
                    print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ–‡æœ¬")
                    continue

                # ç¿»è¯‘
                print("ğŸ”„ æ­£åœ¨ç¿»è¯‘...")
                result = inference.translate(src_text)
                print(f"âœ… ç¿»è¯‘ç»“æœ: {result}")

            except KeyboardInterrupt:
                print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œé€€å‡ºç¨‹åº")
                break
            except Exception as e:
                print(f"âŒ ç¿»è¯‘å‡ºé”™: {e}")
                logger.error(f"ç¿»è¯‘å‡ºé”™: {e}")

    except Exception as e:
        print(f"âŒ æ¨ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        logger.error(f"æ¨ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")


if __name__ == "__main__":
    # æµ‹è¯•æ¨ç†å™¨ - ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹è·¯å¾„
    import os
    model_path = os.path.join(TRAINING_CONFIG.model_save_dir, "best_model.pt")
    print(f"ä½¿ç”¨æ¨¡å‹è·¯å¾„: {model_path}")
    interactive_translation(model_path)
