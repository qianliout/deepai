"""
Transformer2é…ç½®æ–‡ä»¶ - ä½¿ç”¨Pydanticå®šä¹‰æ‰€æœ‰æ•°æ®ç»“æ„å’Œè¶…å‚æ•°
é‡æ„ç‰ˆæœ¬ï¼Œæ¶ˆé™¤é‡å¤ä»£ç ï¼Œä¼˜åŒ–ç»“æ„ï¼Œå‚è€ƒbert2å®ç°æ–¹å¼
è¯¦ç»†çš„æ•°æ®æµè½¬æ³¨é‡Šå’Œshapeè¯´æ˜
"""

import torch
from pydantic import BaseModel, Field
from typing import Optional
import logging
import os
from datetime import datetime


class TransformerConfig(BaseModel):
    """Transformeræ¨¡å‹é…ç½®

    å®šä¹‰æ¨¡å‹æ¶æ„çš„æ‰€æœ‰è¶…å‚æ•°ï¼ŒåŒ…æ‹¬ï¼š
    - æ¨¡å‹ç»´åº¦å’Œå±‚æ•°é…ç½®
    - æ³¨æ„åŠ›æœºåˆ¶é…ç½®
    - æ­£åˆ™åŒ–å‚æ•°
    - ç‰¹æ®Štokenå®šä¹‰

    æ•°æ®æµè½¬è¯´æ˜ï¼š
    è¾“å…¥åºåˆ— [batch_size, seq_len] ->
    Embedding [batch_size, seq_len, d_model] ->
    Encoder/Decoder [batch_size, seq_len, d_model] ->
    è¾“å‡ºæŠ•å½± [batch_size, seq_len, vocab_size]
    """

    # æ¨¡å‹æ¶æ„å‚æ•°
    vocab_size_src: int = Field(default=10000, description="æºè¯­è¨€è¯æ±‡è¡¨å¤§å°")
    vocab_size_tgt: int = Field(default=10000, description="ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨å¤§å°")
    d_model: int = Field(default=512, description="æ¨¡å‹ç»´åº¦ (embeddingç»´åº¦)")
    n_heads: int = Field(default=8, description="å¤šå¤´æ³¨æ„åŠ›å¤´æ•°ï¼Œd_modelå¿…é¡»èƒ½è¢«n_headsæ•´é™¤")
    n_layers: int = Field(default=6, description="ç¼–ç å™¨/è§£ç å™¨å±‚æ•°")
    d_ff: int = Field(default=2048, description="å‰é¦ˆç½‘ç»œä¸­é—´å±‚ç»´åº¦ï¼Œé€šå¸¸æ˜¯d_modelçš„4å€")

    # åºåˆ—é•¿åº¦é…ç½®
    max_seq_len: int = Field(default=512, description="æœ€å¤§åºåˆ—é•¿åº¦ï¼Œå½±å“ä½ç½®ç¼–ç ")

    # æ­£åˆ™åŒ–å‚æ•°
    dropout: float = Field(default=0.1, description="Dropoutæ¦‚ç‡ï¼Œåº”ç”¨äºæ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œ")
    layer_norm_eps: float = Field(default=1e-6, description="LayerNormçš„epsilonå€¼ï¼Œé˜²æ­¢é™¤é›¶")

    # æƒé‡åˆå§‹åŒ–å‚æ•°
    init_range: float = Field(default=0.02, description="æƒé‡åˆå§‹åŒ–èŒƒå›´ï¼Œä½¿ç”¨æ­£æ€åˆ†å¸ƒ")

    # ç‰¹æ®Štoken IDå®šä¹‰
    pad_token_id: int = Field(default=0, description="å¡«å……token ID")
    bos_token_id: int = Field(default=1, description="åºåˆ—å¼€å§‹token ID")
    eos_token_id: int = Field(default=2, description="åºåˆ—ç»“æŸtoken ID")
    unk_token_id: int = Field(default=3, description="æœªçŸ¥token ID")

    class Config:
        """Pydanticé…ç½®"""

        extra = "forbid"  # ç¦æ­¢é¢å¤–å­—æ®µ


class TrainingConfig(BaseModel):
    """è®­ç»ƒé…ç½®

    å®šä¹‰è®­ç»ƒè¿‡ç¨‹çš„æ‰€æœ‰å‚æ•°ï¼ŒåŒ…æ‹¬ï¼š
    - åŸºç¡€è®­ç»ƒå‚æ•°ï¼ˆbatch size, learning rateç­‰ï¼‰
    - ä¼˜åŒ–å™¨é…ç½®
    - æ•°æ®é›†é…ç½®
    - è®¾å¤‡å’Œå¹¶è¡Œé…ç½®
    - æ—¥å¿—å’Œä¿å­˜é…ç½®

    æ•°æ®æµè½¬è¯´æ˜ï¼š
    è®­ç»ƒæ‰¹æ¬¡ [batch_size, seq_len] ->
    æ¨¡å‹å‰å‘ä¼ æ’­ [batch_size, seq_len, vocab_size] ->
    æŸå¤±è®¡ç®— [batch_size, seq_len] ->
    åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°
    """

    # åŸºç¡€è®­ç»ƒå‚æ•°
    batch_size: int = Field(default=32, description="è®­ç»ƒæ‰¹æ¬¡å¤§å°ï¼Œå½±å“å†…å­˜ä½¿ç”¨å’Œæ¢¯åº¦ç¨³å®šæ€§")
    learning_rate: float = Field(default=1e-4, description="åˆå§‹å­¦ä¹ ç‡ï¼Œä¼šé€šè¿‡warmupè°ƒåº¦")
    num_epochs: int = Field(default=1, description="è®­ç»ƒè½®æ•°")
    warmup_steps: int = Field(default=4000, description="å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°ï¼Œè®ºæ–‡å»ºè®®4000")

    # ä¼˜åŒ–å™¨å‚æ•°
    weight_decay: float = Field(default=0.01, description="æƒé‡è¡°å‡ç³»æ•°ï¼ŒL2æ­£åˆ™åŒ–")
    adam_beta1: float = Field(default=0.9, description="Adamä¼˜åŒ–å™¨beta1å‚æ•°ï¼Œä¸€é˜¶çŸ©ä¼°è®¡")
    adam_beta2: float = Field(default=0.98, description="Adamä¼˜åŒ–å™¨beta2å‚æ•°ï¼ŒäºŒé˜¶çŸ©ä¼°è®¡")
    adam_eps: float = Field(default=1e-9, description="Adamä¼˜åŒ–å™¨epsilonå‚æ•°ï¼Œé˜²æ­¢é™¤é›¶")
    max_grad_norm: float = Field(default=1.0, description="æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸")

    # æ•°æ®é›†å‚æ•°
    dataset_name: str = Field(default="Helsinki-NLP/opus_books", description="HuggingFaceæ•°æ®é›†åç§°")
    language_pair: str = Field(default="en-it", description="è¯­è¨€å¯¹ (æºè¯­è¨€-ç›®æ ‡è¯­è¨€)")
    max_samples: Optional[int] = Field(default=1000, description="æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰")
    train_split: str = Field(default="train", description="è®­ç»ƒé›†åˆ†å‰²åç§°")
    val_split: str = Field(default="validation", description="éªŒè¯é›†åˆ†å‰²åç§°")

    # è®¾å¤‡å’Œå¹¶è¡Œé…ç½®
    device: str = Field(default="auto", description="è®­ç»ƒè®¾å¤‡ (auto/cpu/cuda/mps)")
    num_workers: int = Field(default=0, description="æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•° (Mac M1å»ºè®®è®¾ä¸º0)")

    # æ—¥å¿—å’Œä¿å­˜é…ç½®
    logging_steps: int = Field(default=100, description="æ¯å¤šå°‘æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—")
    save_steps: int = Field(default=1000, description="æ¯å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹")
    eval_steps: int = Field(default=500, description="æ¯å¤šå°‘æ­¥è¿›è¡Œä¸€æ¬¡éªŒè¯")

    # ç›®å½•é…ç½® - æ‰€æœ‰è·¯å¾„éƒ½åœ¨è¿™é‡Œç»Ÿä¸€å®šä¹‰
    model_save_dir: str = Field(default="/Users/liuqianli/work/python/deepai/saved_model/transformer2", description="æ¨¡å‹ä¿å­˜ç›®å½•")
    log_dir: str = Field(default="/Users/liuqianli/work/python/deepai/logs/transformer2", description="æ—¥å¿—ä¿å­˜ç›®å½•")
    cache_dir: str = Field(default="/Users/liuqianli/.cache/huggingface/datasets", description="HuggingFaceæ•°æ®é›†ç¼“å­˜ç›®å½•")
    vocab_save_dir: str = Field(default="/Users/liuqianli/work/python/deepai/saved_model/transformer2/vocab", description="è¯æ±‡è¡¨ä¿å­˜ç›®å½•")

    class Config:
        """Pydanticé…ç½®"""

        extra = "forbid"


class DataConfig(BaseModel):
    """æ•°æ®é…ç½®

    å®šä¹‰æ•°æ®é¢„å¤„ç†çš„æ‰€æœ‰å‚æ•°ï¼ŒåŒ…æ‹¬ï¼š
    - åˆ†è¯å™¨é…ç½®
    - æ•°æ®é¢„å¤„ç†å‚æ•°
    - æ•°æ®å¢å¼ºé…ç½®

    æ•°æ®æµè½¬è¯´æ˜ï¼š
    åŸå§‹æ–‡æœ¬ -> åˆ†è¯ -> tokenåºåˆ— [seq_len] ->
    æ·»åŠ ç‰¹æ®Štoken -> å¡«å……/æˆªæ–­ [max_length] ->
    è½¬æ¢ä¸ºIDåºåˆ— [max_length] -> æ‰¹æ¬¡ [batch_size, max_length]
    """

    # åˆ†è¯å™¨é…ç½®
    tokenizer_type: str = Field(default="simple", description="åˆ†è¯å™¨ç±»å‹ (simple/sentencepiece)")
    vocab_file_src: Optional[str] = Field(default=None, description="æºè¯­è¨€è¯æ±‡è¡¨æ–‡ä»¶è·¯å¾„")
    vocab_file_tgt: Optional[str] = Field(default=None, description="ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨æ–‡ä»¶è·¯å¾„")
    min_freq: int = Field(default=2, description="è¯æ±‡æœ€å°é¢‘ç‡é˜ˆå€¼ï¼Œä½äºæ­¤é¢‘ç‡çš„è¯è¢«æ›¿æ¢ä¸ºUNK")

    # æ•°æ®é¢„å¤„ç†å‚æ•°
    max_length: int = Field(default=128, description="åºåˆ—æœ€å¤§é•¿åº¦ï¼Œè¶…è¿‡ä¼šè¢«æˆªæ–­")
    min_length: int = Field(default=1, description="åºåˆ—æœ€å°é•¿åº¦ï¼Œè¿‡çŸ­çš„åºåˆ—ä¼šè¢«è¿‡æ»¤")

    # æ•°æ®å¢å¼ºé…ç½®
    use_data_augmentation: bool = Field(default=False, description="æ˜¯å¦ä½¿ç”¨æ•°æ®å¢å¼º")

    class Config:
        """Pydanticé…ç½®"""

        extra = "forbid"


# åˆ›å»ºå…¨å±€é…ç½®å®ä¾‹ï¼Œæ‰€æœ‰æ¨¡å—éƒ½ä½¿ç”¨è¿™äº›å®ä¾‹ï¼Œæ— éœ€æ‰‹åŠ¨ä¼ å‚
TRANSFORMER_CONFIG = TransformerConfig()
TRAINING_CONFIG = TrainingConfig()
DATA_CONFIG = DataConfig()


# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================


def get_device() -> str:
    """è‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„è®¡ç®—è®¾å¤‡

    Returns:
        str: è®¾å¤‡åç§° ('mps'/'cuda'/'cpu')
    """
    if TRAINING_CONFIG.device == "auto":
        if torch.backends.mps.is_available():
            return "mps"
        elif torch.cuda.is_available():
            return "cuda"
        else:
            return "cpu"
    else:
        return TRAINING_CONFIG.device


def setup_logging():
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ

    åˆ›å»ºç»Ÿä¸€çš„æ—¥å¿—é…ç½®ï¼ŒåŒ…æ‹¬ï¼š
    - æ§åˆ¶å°è¾“å‡º
    - æ–‡ä»¶è¾“å‡º
    - ç¬¬ä¸‰æ–¹åº“æ—¥å¿—çº§åˆ«æ§åˆ¶
    """
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    log_dir = TRAINING_CONFIG.log_dir
    os.makedirs(log_dir, exist_ok=True)

    # è®¾ç½®æ—¥å¿—æ ¼å¼
    log_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

    # è®¾ç½®æ ¹æ—¥å¿—å™¨
    logging.basicConfig(
        level=logging.INFO,
        format=log_format,
        handlers=[
            logging.StreamHandler(),  # æ§åˆ¶å°è¾“å‡º
            logging.FileHandler(
                os.path.join(
                    log_dir,
                    f"transformer2_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log",
                ),
                encoding="utf-8",
            ),
        ],
    )

    # è®¾ç½®ç¬¬ä¸‰æ–¹åº“æ—¥å¿—çº§åˆ«ï¼Œé¿å…è¿‡å¤šè¾“å‡º
    logging.getLogger("transformers").setLevel(logging.WARNING)
    logging.getLogger("datasets").setLevel(logging.WARNING)
    logging.getLogger("urllib3").setLevel(logging.WARNING)
    logging.getLogger("filelock").setLevel(logging.WARNING)

    return logging.getLogger("Transformer2")


def create_directories():
    """åˆ›å»ºæ‰€æœ‰å¿…è¦çš„ç›®å½•"""
    directories = [
        TRAINING_CONFIG.model_save_dir,
        TRAINING_CONFIG.log_dir,
        TRAINING_CONFIG.cache_dir,
        TRAINING_CONFIG.vocab_save_dir,
    ]

    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        print(f"âœ… åˆ›å»ºç›®å½•: {directory}")


def print_config():
    """æ‰“å°å½“å‰é…ç½®ä¿¡æ¯

    ä»¥ç¾è§‚çš„æ ¼å¼æ˜¾ç¤ºæ‰€æœ‰é…ç½®å‚æ•°ï¼Œä¾¿äºè°ƒè¯•å’Œç¡®è®¤
    """
    print("\n" + "=" * 60)
    print("ğŸ”§ Transformer2 é…ç½®ä¿¡æ¯")
    print("=" * 60)

    print(f"\nğŸ“Š æ¨¡å‹é…ç½®:")
    print(f"  æºè¯­è¨€è¯æ±‡è¡¨å¤§å°: {TRANSFORMER_CONFIG.vocab_size_src:,}")
    print(f"  ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨å¤§å°: {TRANSFORMER_CONFIG.vocab_size_tgt:,}")
    print(f"  æ¨¡å‹ç»´åº¦ (d_model): {TRANSFORMER_CONFIG.d_model}")
    print(f"  æ³¨æ„åŠ›å¤´æ•°: {TRANSFORMER_CONFIG.n_heads}")
    print(f"  ç¼–ç å™¨/è§£ç å™¨å±‚æ•°: {TRANSFORMER_CONFIG.n_layers}")
    print(f"  å‰é¦ˆç½‘ç»œç»´åº¦: {TRANSFORMER_CONFIG.d_ff}")
    print(f"  æœ€å¤§åºåˆ—é•¿åº¦: {TRANSFORMER_CONFIG.max_seq_len}")
    print(f"  Dropoutæ¦‚ç‡: {TRANSFORMER_CONFIG.dropout}")

    print(f"\nğŸš€ è®­ç»ƒé…ç½®:")
    print(f"  æ‰¹æ¬¡å¤§å°: {TRAINING_CONFIG.batch_size}")
    print(f"  å­¦ä¹ ç‡: {TRAINING_CONFIG.learning_rate}")
    print(f"  è®­ç»ƒè½®æ•°: {TRAINING_CONFIG.num_epochs}")
    print(f"  é¢„çƒ­æ­¥æ•°: {TRAINING_CONFIG.warmup_steps}")
    print(f"  æœ€å¤§æ ·æœ¬æ•°: {TRAINING_CONFIG.max_samples}")
    print(f"  è®¡ç®—è®¾å¤‡: {get_device()}")
    print(f"  æ¨¡å‹ä¿å­˜ç›®å½•: {TRAINING_CONFIG.model_save_dir}")
    print(f"  æ—¥å¿—ä¿å­˜ç›®å½•: {TRAINING_CONFIG.log_dir}")
    print(f"  æ•°æ®ç¼“å­˜ç›®å½•: {TRAINING_CONFIG.cache_dir}")

    print(f"\nğŸ“ æ•°æ®é…ç½®:")
    print(f"  æ•°æ®é›†: {TRAINING_CONFIG.dataset_name}")
    print(f"  è¯­è¨€å¯¹: {TRAINING_CONFIG.language_pair}")
    print(f"  æœ€å¤§åºåˆ—é•¿åº¦: {DATA_CONFIG.max_length}")
    print(f"  æœ€å°è¯é¢‘: {DATA_CONFIG.min_freq}")
    print(f"  åˆ†è¯å™¨ç±»å‹: {DATA_CONFIG.tokenizer_type}")

    print("=" * 60 + "\n")


def update_config_for_quick_test():
    """æ›´æ–°é…ç½®ä¸ºå¿«é€Ÿæµ‹è¯•æ¨¡å¼

    å°†é…ç½®ä¿®æ”¹ä¸ºå°è§„æ¨¡è®¾ç½®ï¼Œç”¨äºå¿«é€ŸéªŒè¯ä»£ç æ­£ç¡®æ€§ï¼š
    - å‡å°æ¨¡å‹è§„æ¨¡
    - å‡å°‘è®­ç»ƒæ•°æ®é‡
    - ç¼©çŸ­åºåˆ—é•¿åº¦
    """
    print("âš¡ åˆ‡æ¢åˆ°å¿«é€Ÿæµ‹è¯•æ¨¡å¼...")

    # å°è§„æ¨¡æ¨¡å‹é…ç½®
    TRANSFORMER_CONFIG.d_model = 256
    TRANSFORMER_CONFIG.n_heads = 4
    TRANSFORMER_CONFIG.n_layers = 3
    TRANSFORMER_CONFIG.d_ff = 1024
    TRANSFORMER_CONFIG.vocab_size_src = 5000
    TRANSFORMER_CONFIG.vocab_size_tgt = 5000

    # å¿«é€Ÿè®­ç»ƒé…ç½®
    TRAINING_CONFIG.batch_size = 16
    TRAINING_CONFIG.num_epochs = 2
    TRAINING_CONFIG.max_samples = 100
    TRAINING_CONFIG.model_save_dir = "/Users/liuqianli/work/python/deepai/saved_model/transformer2_quick_test"
    TRAINING_CONFIG.log_dir = "/Users/liuqianli/work/python/deepai/logs/transformer2_quick_test"
    TRAINING_CONFIG.logging_steps = 10
    TRAINING_CONFIG.save_steps = 50
    TRAINING_CONFIG.eval_steps = 25

    # çŸ­åºåˆ—é…ç½®
    DATA_CONFIG.max_length = 64
    TRANSFORMER_CONFIG.max_seq_len = 64

    print("âœ… å·²åˆ‡æ¢åˆ°å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
    print(f"  - æ¨¡å‹ç»´åº¦: {TRANSFORMER_CONFIG.d_model}")
    print(f"  - è®­ç»ƒæ ·æœ¬: {TRAINING_CONFIG.max_samples}")
    print(f"  - åºåˆ—é•¿åº¦: {DATA_CONFIG.max_length}")


def validate_config():
    """éªŒè¯é…ç½®çš„åˆç†æ€§

    æ£€æŸ¥é…ç½®å‚æ•°æ˜¯å¦ç¬¦åˆè¦æ±‚ï¼Œé˜²æ­¢è¿è¡Œæ—¶é”™è¯¯
    """
    # æ£€æŸ¥æ¨¡å‹é…ç½®
    assert (
        TRANSFORMER_CONFIG.d_model % TRANSFORMER_CONFIG.n_heads == 0
    ), f"d_model({TRANSFORMER_CONFIG.d_model})å¿…é¡»èƒ½è¢«n_heads({TRANSFORMER_CONFIG.n_heads})æ•´é™¤"

    assert TRANSFORMER_CONFIG.d_model > 0, "d_modelå¿…é¡»å¤§äº0"
    assert TRANSFORMER_CONFIG.n_heads > 0, "n_headså¿…é¡»å¤§äº0"
    assert TRANSFORMER_CONFIG.n_layers > 0, "n_layerså¿…é¡»å¤§äº0"
    assert TRANSFORMER_CONFIG.d_ff > 0, "d_ffå¿…é¡»å¤§äº0"

    # æ£€æŸ¥è®­ç»ƒé…ç½®
    assert TRAINING_CONFIG.batch_size > 0, "batch_sizeå¿…é¡»å¤§äº0"
    assert TRAINING_CONFIG.learning_rate > 0, "learning_rateå¿…é¡»å¤§äº0"
    assert TRAINING_CONFIG.num_epochs > 0, "num_epochså¿…é¡»å¤§äº0"

    # æ£€æŸ¥æ•°æ®é…ç½®
    assert DATA_CONFIG.max_length > 0, "max_lengthå¿…é¡»å¤§äº0"
    assert DATA_CONFIG.min_length > 0, "min_lengthå¿…é¡»å¤§äº0"
    assert DATA_CONFIG.max_length >= DATA_CONFIG.min_length, "max_lengthå¿…é¡»å¤§äºç­‰äºmin_length"

    print("âœ… é…ç½®éªŒè¯é€šè¿‡")


# ============================================================================
# è‡ªåŠ¨åˆå§‹åŒ–
# ============================================================================

# è‡ªåŠ¨è®¾ç½®è®¾å¤‡
if TRAINING_CONFIG.device == "auto":
    TRAINING_CONFIG.device = get_device()
    print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹åˆ°è®¾å¤‡: {TRAINING_CONFIG.device}")

# éªŒè¯é…ç½®
validate_config()


if __name__ == "__main__":
    # æµ‹è¯•é…ç½®
    print_config()

    # æµ‹è¯•è®¾å¤‡æ£€æµ‹
    device = get_device()
    print(f"\næ£€æµ‹åˆ°çš„è®¾å¤‡: {device}")

    # æµ‹è¯•æ—¥å¿—
    logger = setup_logging()
    logger.info("é…ç½®æ¨¡å—æµ‹è¯•æˆåŠŸï¼")

    # æµ‹è¯•å¿«é€Ÿæ¨¡å¼
    print("\næµ‹è¯•å¿«é€Ÿæ¨¡å¼:")
    update_config_for_quick_test()
    print_config()
