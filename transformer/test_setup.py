#!/usr/bin/env python3
"""
æµ‹è¯•è„šæœ¬ - éªŒè¯ä»£ç è®¾ç½®æ˜¯å¦æ­£ç¡®
"""
import sys
import traceback


def test_imports():
    """æµ‹è¯•æ‰€æœ‰æ¨¡å—å¯¼å…¥"""
    print("æµ‹è¯•æ¨¡å—å¯¼å…¥...")

    try:
        import torch

        print("âœ“ PyTorch")

        # æµ‹è¯•è®¾å¤‡
        if torch.backends.mps.is_available():
            print("âœ“ MPS (Apple Silicon GPU) å¯ç”¨")
        elif torch.cuda.is_available():
            print("âœ“ CUDA å¯ç”¨")
        else:
            print("âœ“ CPU æ¨¡å¼")

    except ImportError as e:
        print(f"âœ— PyTorch å¯¼å…¥å¤±è´¥: {e}")
        return False

    try:
        from pydantic import BaseModel

        print("âœ“ Pydantic")
    except ImportError as e:
        print(f"âœ— Pydantic å¯¼å…¥å¤±è´¥: {e}")
        return False

    try:
        import datasets

        print("âœ“ Datasets")
    except ImportError as e:
        print(f"âœ— Datasets å¯¼å…¥å¤±è´¥: {e}")
        return False

    try:
        import numpy

        print("âœ“ NumPy")
    except ImportError as e:
        print(f"âœ— NumPy å¯¼å…¥å¤±è´¥: {e}")
        return False

    return True


def test_local_imports():
    """æµ‹è¯•æœ¬åœ°æ¨¡å—å¯¼å…¥"""
    print("\næµ‹è¯•æœ¬åœ°æ¨¡å—å¯¼å…¥...")

    modules = ["config", "utils", "tokenizer", "model", "data_loader", "trainer"]

    for module_name in modules:
        try:
            __import__(module_name)
            print(f"âœ“ {module_name}")
        except ImportError as e:
            print(f"âœ— {module_name} å¯¼å…¥å¤±è´¥: {e}")
            traceback.print_exc()
            return False

    return True


def test_config():
    """æµ‹è¯•é…ç½®"""
    print("\næµ‹è¯•é…ç½®...")

    try:
        from config import Config, default_config

        # æµ‹è¯•é»˜è®¤é…ç½®
        config = default_config
        print(f"âœ“ é»˜è®¤é…ç½®åŠ è½½æˆåŠŸ")
        print(f"  - æ¨¡å‹ç»´åº¦: {config.model.d_model}")
        print(f"  - æ³¨æ„åŠ›å¤´æ•°: {config.model.n_heads}")
        print(f"  - è®¾å¤‡: {config.training.device}")

        return True

    except Exception as e:
        print(f"âœ— é…ç½®æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_model_creation():
    """æµ‹è¯•æ¨¡å‹åˆ›å»º"""
    print("\næµ‹è¯•æ¨¡å‹åˆ›å»º...")

    try:
        from config import default_config
        from model import Transformer

        # åˆ›å»ºå°å‹æ¨¡å‹ç”¨äºæµ‹è¯•
        config = default_config
        config.model.d_model = 128
        config.model.n_heads = 4
        config.model.n_layers = 2
        config.model.vocab_size_en = 1000
        config.model.vocab_size_it = 1000

        model = Transformer(config.model)
        print(f"âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")

        # è®¡ç®—å‚æ•°æ•°é‡
        from utils import count_parameters

        param_count = count_parameters(model)
        print(f"  - å‚æ•°æ•°é‡: {param_count:,}")

        return True

    except Exception as e:
        print(f"âœ— æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_tokenizer():
    """æµ‹è¯•åˆ†è¯å™¨"""
    print("\næµ‹è¯•åˆ†è¯å™¨...")

    try:
        from config import default_config
        from tokenizer import SimpleTokenizer

        tokenizer = SimpleTokenizer(default_config.model)

        # æµ‹è¯•åˆ†è¯
        text = "Hello, how are you?"
        tokens = tokenizer.tokenize(text)
        print(f"âœ“ åˆ†è¯æµ‹è¯•: '{text}' -> {tokens}")

        # æµ‹è¯•è¯å…¸æ„å»º
        texts = ["Hello world", "How are you", "I am fine", "Thank you very much"]

        vocab = tokenizer.build_vocab(texts, "en", min_freq=1, max_vocab_size=100)
        print(f"âœ“ è¯å…¸æ„å»ºæˆåŠŸï¼Œå¤§å°: {len(vocab)}")

        return True

    except Exception as e:
        print(f"âœ— åˆ†è¯å™¨æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_forward_pass():
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    print("\næµ‹è¯•å‰å‘ä¼ æ’­...")

    try:
        import torch
        from config import default_config
        from model import Transformer

        # åˆ›å»ºå°å‹æ¨¡å‹
        config = default_config
        config.model.d_model = 64
        config.model.n_heads = 4
        config.model.n_layers = 2
        config.model.vocab_size_en = 100
        config.model.vocab_size_it = 100
        config.model.max_seq_len = 10

        model = Transformer(config.model)
        model.eval()

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 2
        src_len = 8
        tgt_len = 6

        src = torch.randint(1, 100, (batch_size, src_len))
        tgt = torch.randint(1, 100, (batch_size, tgt_len))

        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            output = model(src, tgt)

        print(f"âœ“ å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"  - è¾“å…¥å½¢çŠ¶: src={src.shape}, tgt={tgt.shape}")
        print(f"  - è¾“å‡ºå½¢çŠ¶: {output.shape}")

        return True

    except Exception as e:
        print(f"âœ— å‰å‘ä¼ æ’­æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 60)
    print("Transformerå®ç° - è®¾ç½®éªŒè¯")
    print("=" * 60)

    tests = [
        test_imports,
        test_local_imports,
        test_config,
        test_model_creation,
        test_tokenizer,
        test_forward_pass,
    ]

    passed = 0
    total = len(tests)

    for test in tests:
        try:
            if test():
                passed += 1
            else:
                print("æµ‹è¯•å¤±è´¥ï¼Œåœæ­¢åç»­æµ‹è¯•")
                break
        except Exception as e:
            print(f"æµ‹è¯•å¼‚å¸¸: {e}")
            traceback.print_exc()
            break

    print("\n" + "=" * 60)
    print(f"æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")

    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ä»£ç è®¾ç½®æ­£ç¡®ã€‚")
        print("\nå¯ä»¥å¼€å§‹è®­ç»ƒ:")
        print("  python main.py --mode train")
        print("\næˆ–ä½¿ç”¨ä¸€é”®è„šæœ¬:")
        print("  python run.py")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¾èµ–å’Œä»£ç ã€‚")
        print("\nå®‰è£…ä¾èµ–:")
        print("  pip install -r requirements.txt")

    print("=" * 60)


if __name__ == "__main__":
    main()
