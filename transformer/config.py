"""
é…ç½®æ–‡ä»¶ - ä½¿ç”¨Pydanticå®šä¹‰æ‰€æœ‰æ•°æ®ç»“æ„å’Œè¶…å‚æ•°
"""

from pydantic import BaseModel, Field
from typing import Optional, List
import torch


class ModelConfig(BaseModel):
    """Transformeræ¨¡å‹é…ç½®"""

    # æ¨¡å‹ç»´åº¦
    d_model: int = Field(default=512, description="æ¨¡å‹éšè—å±‚ç»´åº¦")
    d_ff: int = Field(default=2048, description="å‰é¦ˆç½‘ç»œç»´åº¦")
    n_heads: int = Field(default=8, description="å¤šå¤´æ³¨æ„åŠ›å¤´æ•°")
    n_layers: int = Field(default=6, description="ç¼–ç å™¨/è§£ç å™¨å±‚æ•°")

    # è¯æ±‡è¡¨
    vocab_size_en: int = Field(default=10000, description="è‹±è¯­è¯æ±‡è¡¨å¤§å°")
    vocab_size_it: int = Field(default=10000, description="æ„å¤§åˆ©è¯­è¯æ±‡è¡¨å¤§å°")
    max_seq_len: int = Field(default=128, description="æœ€å¤§åºåˆ—é•¿åº¦") # ä¹Ÿå°±transformerä¸­çš„seq_len

    # æ­£åˆ™åŒ–
    dropout: float = Field(default=0.1, description="Dropoutæ¦‚ç‡")

    # ç‰¹æ®Štoken
    pad_token: str = Field(default="<PAD>", description="å¡«å……token")
    unk_token: str = Field(default="<UNK>", description="æœªçŸ¥token")
    bos_token: str = Field(default="<BOS>", description="å¥å­å¼€å§‹token")
    eos_token: str = Field(default="<EOS>", description="å¥å­ç»“æŸtoken")


class TrainingConfig(BaseModel):
    """è®­ç»ƒé…ç½®"""

    # æ•°æ®
    train_size: int = Field(default=10000, description="è®­ç»ƒæ•°æ®å¤§å°")
    val_size: int = Field(default=2000, description="éªŒè¯æ•°æ®å¤§å°")
    batch_size: int = Field(default=32, description="æ‰¹æ¬¡å¤§å°")

    # è®­ç»ƒå‚æ•°
    learning_rate: float = Field(default=1e-4, description="å­¦ä¹ ç‡")
    num_epochs: int = Field(default=10, description="è®­ç»ƒè½®æ•°")
    warmup_steps: int = Field(default=4000, description="å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°")

    # è®¾å¤‡é…ç½®
    device: str = Field(
        default="mps" if torch.backends.mps.is_available() else "cpu",
        description="è®­ç»ƒè®¾å¤‡",
    )

    # ç›®å½•é…ç½® - æ‰€æœ‰è·¯å¾„éƒ½åœ¨è¿™é‡Œç»Ÿä¸€å®šä¹‰
    # é¢„è®­ç»ƒç›¸å…³ç›®å½•
    pretrain_checkpoints_dir: str = Field(default="/Users/liuqianli/work/python/deepai/saved_model/transformer/pretrain/checkpoints", description="é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¨¡å‹ä¿å­˜ç›®å½•")
    pretrain_best_dir: str = Field(default="/Users/liuqianli/work/python/deepai/saved_model/transformer/pretrain/best", description="é¢„è®­ç»ƒæœ€ä½³æ¨¡å‹ä¿å­˜ç›®å½•")
    pretrain_final_dir: str = Field(default="/Users/liuqianli/work/python/deepai/saved_model/transformer/pretrain/final", description="é¢„è®­ç»ƒå®Œæˆåæœ€ç»ˆæ¨¡å‹ä¿å­˜ç›®å½•")
    pretrain_vocab_dir: str = Field(default="/Users/liuqianli/work/python/deepai/saved_model/transformer/pretrain/vocab", description="å­—å…¸å­˜æ”¾ç›®å½•")

    # å…¶ä»–ç›®å½•
    log_dir: str = Field(default="/Users/liuqianli/work/python/deepai/logs/transformer", description="æ—¥å¿—ä¿å­˜ç›®å½•")
    cache_dir: str = Field(default="/Users/liuqianli/.cache/huggingface/datasets", description="HuggingFaceæ•°æ®é›†ç¼“å­˜ç›®å½•")

    # æ—¥å¿—å’Œä¿å­˜
    log_interval: int = Field(default=100, description="æ—¥å¿—æ‰“å°é—´éš”")
    save_interval: int = Field(default=1000, description="æ¨¡å‹ä¿å­˜é—´éš”")


class DataConfig(BaseModel):
    """æ•°æ®é…ç½®"""

    dataset_name: str = Field(
        default="Helsinki-NLP/opus_books", description="æ•°æ®é›†åç§°"
    )
    language_pair: str = Field(default="en-it", description="è¯­è¨€å¯¹")

    # åˆ†è¯é…ç½®
    min_freq: int = Field(default=4, description="è¯æ±‡æœ€å°é¢‘ç‡")
    max_vocab_size: int = Field(default=10000, description="æœ€å¤§è¯æ±‡è¡¨å¤§å°")


# å…¨å±€é…ç½®å®ä¾‹
MODEL_CONFIG = ModelConfig()
TRAINING_CONFIG = TrainingConfig()
DATA_CONFIG = DataConfig()


def create_directories():
    """åˆ›å»ºæ‰€æœ‰å¿…è¦çš„ç›®å½•"""
    import os

    directories = [
        TRAINING_CONFIG.pretrain_checkpoints_dir,
        TRAINING_CONFIG.pretrain_best_dir,
        TRAINING_CONFIG.pretrain_final_dir,
        TRAINING_CONFIG.pretrain_vocab_dir,
        TRAINING_CONFIG.log_dir,
        TRAINING_CONFIG.cache_dir,
    ]

    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        print(f"âœ… åˆ›å»ºç›®å½•: {directory}")


def get_device():
    """è·å–è®¾å¤‡"""
    if TRAINING_CONFIG.device == "auto":
        if torch.cuda.is_available():
            return torch.device("cuda")
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            return torch.device("mps")
        else:
            return torch.device("cpu")
    else:
        return torch.device(TRAINING_CONFIG.device)


def print_config():
    """æ‰“å°é…ç½®ä¿¡æ¯"""
    print("=" * 60)
    print("Transformeré…ç½®ä¿¡æ¯")
    print("=" * 60)

    print("\nğŸ—ï¸ æ¨¡å‹é…ç½®:")
    print(f"  æ¨¡å‹ç»´åº¦: {MODEL_CONFIG.d_model}")
    print(f"  æ³¨æ„åŠ›å¤´æ•°: {MODEL_CONFIG.n_heads}")
    print(f"  ç¼–ç å™¨/è§£ç å™¨å±‚æ•°: {MODEL_CONFIG.n_layers}")
    print(f"  å‰é¦ˆç½‘ç»œç»´åº¦: {MODEL_CONFIG.d_ff}")
    print(f"  æœ€å¤§åºåˆ—é•¿åº¦: {MODEL_CONFIG.max_seq_len}")
    print(f"  Dropout: {MODEL_CONFIG.dropout}")

    print("\nğŸš€ è®­ç»ƒé…ç½®:")
    print(f"  è®­ç»ƒæ•°æ®å¤§å°: {TRAINING_CONFIG.train_size}")
    print(f"  éªŒè¯æ•°æ®å¤§å°: {TRAINING_CONFIG.val_size}")
    print(f"  æ‰¹æ¬¡å¤§å°: {TRAINING_CONFIG.batch_size}")
    print(f"  å­¦ä¹ ç‡: {TRAINING_CONFIG.learning_rate}")
    print(f"  è®­ç»ƒè½®æ•°: {TRAINING_CONFIG.num_epochs}")
    print(f"  è®¾å¤‡: {get_device()}")

    print("\nğŸ“Š æ•°æ®é…ç½®:")
    print(f"  æ•°æ®é›†: {DATA_CONFIG.dataset_name}")
    print(f"  è¯­è¨€å¯¹: {DATA_CONFIG.language_pair}")
    print(f"  æœ€å°è¯é¢‘: {DATA_CONFIG.min_freq}")
    print(f"  æœ€å¤§è¯æ±‡è¡¨å¤§å°: {DATA_CONFIG.max_vocab_size}")

    print("\nğŸ“ ç›®å½•é…ç½®:")
    print(f"  é¢„è®­ç»ƒæ£€æŸ¥ç‚¹ç›®å½•: {TRAINING_CONFIG.pretrain_checkpoints_dir}")
    print(f"  é¢„è®­ç»ƒæœ€ä½³æ¨¡å‹ç›®å½•: {TRAINING_CONFIG.pretrain_best_dir}")
    print(f"  é¢„è®­ç»ƒæœ€ç»ˆæ¨¡å‹ç›®å½•: {TRAINING_CONFIG.pretrain_final_dir}")
    print(f"  è¯å…¸ä¿å­˜ç›®å½•: {TRAINING_CONFIG.pretrain_vocab_dir}")
    print(f"  æ—¥å¿—ä¿å­˜ç›®å½•: {TRAINING_CONFIG.log_dir}")
    print(f"  æ•°æ®ç¼“å­˜ç›®å½•: {TRAINING_CONFIG.cache_dir}")

    print("=" * 60)


if __name__ == "__main__":
    # æµ‹è¯•é…ç½®
    print_config()
    print("\næµ‹è¯•ç›®å½•åˆ›å»º:")
    create_directories()