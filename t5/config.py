"""
T5é…ç½®æ¨¡å— - ç»Ÿä¸€çš„å‚æ•°ç®¡ç†
æ‰€æœ‰è¶…å‚æ•°éƒ½åœ¨è¿™é‡Œå®šä¹‰ï¼Œè¿è¡Œæ—¶ä¸éœ€è¦æ‰‹åŠ¨ä¼ å‚
åŸºäºpydanticè¿›è¡Œæ•°æ®ç»“æ„å®šä¹‰å’ŒéªŒè¯
"""

import torch
from pydantic import BaseModel, Field
from typing import Optional, List
import logging


class T5Config(BaseModel):
    """T5æ¨¡å‹é…ç½®"""

    # æ¨¡å‹æ¶æ„å‚æ•°
    vocab_size: int = Field(default=32128, description="è¯æ±‡è¡¨å¤§å°")
    d_model: int = Field(default=512, description="æ¨¡å‹ç»´åº¦")
    d_kv: int = Field(default=64, description="é”®å€¼ç»´åº¦")
    d_ff: int = Field(default=2048, description="å‰é¦ˆç½‘ç»œç»´åº¦")
    num_layers: int = Field(default=6, description="ç¼–ç å™¨å’Œè§£ç å™¨å±‚æ•°")
    num_heads: int = Field(default=8, description="æ³¨æ„åŠ›å¤´æ•°")
    
    # ç›¸å¯¹ä½ç½®ç¼–ç 
    relative_attention_num_buckets: int = Field(default=32, description="ç›¸å¯¹ä½ç½®ç¼–ç æ¡¶æ•°")
    relative_attention_max_distance: int = Field(default=128, description="ç›¸å¯¹ä½ç½®ç¼–ç æœ€å¤§è·ç¦»")
    
    # æ­£åˆ™åŒ–å‚æ•°
    dropout_rate: float = Field(default=0.1, description="Dropoutæ¦‚ç‡")
    layer_norm_epsilon: float = Field(default=1e-6, description="LayerNormçš„epsilon")
    
    # åˆå§‹åŒ–å‚æ•°
    initializer_factor: float = Field(default=1.0, description="æƒé‡åˆå§‹åŒ–å› å­")
    use_custom_init: bool = Field(default=True, description="æ˜¯å¦ä½¿ç”¨è‡ªå®šä¹‰æƒé‡åˆå§‹åŒ–")

    # åºåˆ—é•¿åº¦
    max_length: int = Field(default=512, description="æœ€å¤§åºåˆ—é•¿åº¦")
    
    # ä»»åŠ¡ç‰¹å®šå‚æ•°
    decoder_start_token_id: int = Field(default=0, description="è§£ç å™¨å¼€å§‹token id")
    eos_token_id: int = Field(default=1, description="ç»“æŸtoken id")
    pad_token_id: int = Field(default=0, description="padding token id")
    
    class Config:
        """Pydanticé…ç½®"""
        extra = "forbid"  # ç¦æ­¢é¢å¤–å­—æ®µ


class TrainingConfig(BaseModel):
    """è®­ç»ƒé…ç½®"""

    # åŸºç¡€è®­ç»ƒå‚æ•°
    batch_size: int = Field(default=8, description="æ‰¹æ¬¡å¤§å°")
    learning_rate: float = Field(default=5e-5, description="å­¦ä¹ ç‡")
    num_epochs: int = Field(default=3, description="è®­ç»ƒè½®æ•°")
    warmup_steps: int = Field(default=100, description="é¢„çƒ­æ­¥æ•°")
    
    # ä¼˜åŒ–å™¨å‚æ•°
    weight_decay: float = Field(default=0.01, description="æƒé‡è¡°å‡")
    adam_epsilon: float = Field(default=1e-8, description="Adamä¼˜åŒ–å™¨epsilon")
    adam_beta1: float = Field(default=0.9, description="Adamä¼˜åŒ–å™¨beta1")
    adam_beta2: float = Field(default=0.999, description="Adamä¼˜åŒ–å™¨beta2")
    max_grad_norm: float = Field(default=1.0, description="æ¢¯åº¦è£å‰ªé˜ˆå€¼")
    
    # æ•°æ®é›†å‚æ•°
    dataset_name: str = Field(default="simple_test", description="æ•°æ®é›†åç§°")
    dataset_config: str = Field(default="3.0.0", description="æ•°æ®é›†é…ç½®ç‰ˆæœ¬")
    max_samples: Optional[int] = Field(default=1000, description="æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰")
    
    # è®¾å¤‡å’Œå¹¶è¡Œ
    device: str = Field(default="auto", description="è®­ç»ƒè®¾å¤‡")
    num_workers: int = Field(default=4, description="æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°")
    
    # æ—¥å¿—å’Œä¿å­˜
    logging_steps: int = Field(default=100, description="æ—¥å¿—è®°å½•æ­¥æ•°")
    save_steps: int = Field(default=1000, description="æ¨¡å‹ä¿å­˜æ­¥æ•°")
    eval_steps: int = Field(default=500, description="è¯„ä¼°æ­¥æ•°")
    
    # ç›®å½•é…ç½® - æ‰€æœ‰è·¯å¾„éƒ½åœ¨è¿™é‡Œç»Ÿä¸€å®šä¹‰
    # é¢„è®­ç»ƒç›¸å…³ç›®å½•
    pretrain_checkpoints_dir: str = Field(
        default="/Users/liuqianli/work/python/deepai/saved_model/t5/pretrain/checkpoints",
        description="é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•",
    )
    pretrain_best_dir: str = Field(
        default="/Users/liuqianli/work/python/deepai/saved_model/t5/pretrain/best",
        description="é¢„è®­ç»ƒæœ€ä½³æ¨¡å‹ä¿å­˜ç›®å½•",
    )
    pretrain_final_dir: str = Field(
        default="/Users/liuqianli/work/python/deepai/saved_model/t5/pretrain/final",
        description="é¢„è®­ç»ƒæœ€ç»ˆæ¨¡å‹ä¿å­˜ç›®å½•",
    )
    
    # å¾®è°ƒç›¸å…³ç›®å½•
    finetuning_checkpoints_dir: str = Field(
        default="/Users/liuqianli/work/python/deepai/saved_model/t5/finetuning/checkpoints",
        description="å¾®è°ƒè¿‡ç¨‹ä¸­çš„æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•",
    )
    finetuning_best_dir: str = Field(
        default="/Users/liuqianli/work/python/deepai/saved_model/t5/finetuning/best",
        description="å¾®è°ƒæœ€ä½³æ¨¡å‹ä¿å­˜ç›®å½•",
    )
    finetuning_final_dir: str = Field(
        default="/Users/liuqianli/work/python/deepai/saved_model/t5/finetuning/final",
        description="å¾®è°ƒæœ€ç»ˆæ¨¡å‹ä¿å­˜ç›®å½•",
    )
    
    # å…¶ä»–ç›®å½•
    log_dir: str = Field(
        default="/Users/liuqianli/work/python/deepai/logs/t5",
        description="æ—¥å¿—ä¿å­˜ç›®å½•",
    )
    cache_dir: str = Field(
        default="/Users/liuqianli/.cache/huggingface/datasets",
        description="HuggingFaceæ•°æ®é›†ç¼“å­˜ç›®å½•",
    )
    
    class Config:
        """Pydanticé…ç½®"""
        extra = "forbid"


class DataConfig(BaseModel):
    """æ•°æ®é…ç½®"""

    # Tokenizeré…ç½®
    tokenizer_name: str = Field(default="t5-small", description="tokenizeråç§°")

    # åºåˆ—é•¿åº¦
    max_source_length: int = Field(default=512, description="æºåºåˆ—æœ€å¤§é•¿åº¦")
    max_target_length: int = Field(default=128, description="ç›®æ ‡åºåˆ—æœ€å¤§é•¿åº¦")

    # ä»»åŠ¡å‰ç¼€
    task_prefix: str = Field(default="translate English to German: ", description="ä»»åŠ¡å‰ç¼€")

    class Config:
        """Pydanticé…ç½®"""
        extra = "forbid"


class LoggingConfig(BaseModel):
    """æ—¥å¿—é…ç½®"""

    log_level: str = Field(default="INFO", description="æ—¥å¿—çº§åˆ«")
    log_format: str = Field(
        default="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        description="æ—¥å¿—æ ¼å¼",
    )
    log_file: Optional[str] = Field(
        default=None, description="æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼Œå°†ä½¿ç”¨TRAINING_CONFIG.log_dir"
    )
    
    class Config:
        """Pydanticé…ç½®"""
        extra = "forbid"


# å…¨å±€é…ç½®å®ä¾‹ - è¿è¡Œæ—¶ç›´æ¥ä½¿ç”¨è¿™äº›é…ç½®ï¼Œä¸éœ€è¦æ‰‹åŠ¨ä¼ å‚
T5_CONFIG = T5Config()
TRAINING_CONFIG = TrainingConfig()
DATA_CONFIG = DataConfig()
LOGGING_CONFIG = LoggingConfig()


def get_device() -> torch.device:
    """
    è‡ªåŠ¨æ£€æµ‹å¹¶è¿”å›æœ€ä½³è®¾å¤‡
    
    Returns:
        torch.device: è®¾å¤‡å¯¹è±¡
    """
    if TRAINING_CONFIG.device == "auto":
        if torch.cuda.is_available():
            device = torch.device("cuda")
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            device = torch.device("mps")
        else:
            device = torch.device("cpu")
    else:
        device = torch.device(TRAINING_CONFIG.device)
    
    return device


def setup_logging():
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    import os
    from datetime import datetime
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    log_dir = TRAINING_CONFIG.log_dir
    os.makedirs(log_dir, exist_ok=True)
    
    # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶å
    log_filename = f"t5_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    log_filepath = os.path.join(log_dir, log_filename)
    
    logging.basicConfig(
        level=getattr(logging, LOGGING_CONFIG.log_level),
        format=LOGGING_CONFIG.log_format,
        handlers=[
            logging.StreamHandler(),  # æ§åˆ¶å°è¾“å‡º
            logging.FileHandler(log_filepath, encoding="utf-8"),  # æ–‡ä»¶è¾“å‡º
        ],
    )
    
    # åˆ›å»ºT5ä¸“ç”¨logger
    logger = logging.getLogger("T5")
    logger.setLevel(getattr(logging, LOGGING_CONFIG.log_level))
    
    return logger


def create_directories():
    """åˆ›å»ºæ‰€æœ‰å¿…è¦çš„ç›®å½•"""
    import os
    
    directories = [
        # é¢„è®­ç»ƒç›¸å…³ç›®å½•
        TRAINING_CONFIG.pretrain_checkpoints_dir,
        TRAINING_CONFIG.pretrain_best_dir,
        TRAINING_CONFIG.pretrain_final_dir,
        # å¾®è°ƒç›¸å…³ç›®å½•
        TRAINING_CONFIG.finetuning_checkpoints_dir,
        TRAINING_CONFIG.finetuning_best_dir,
        TRAINING_CONFIG.finetuning_final_dir,
        # å…¶ä»–ç›®å½•
        TRAINING_CONFIG.log_dir,
        TRAINING_CONFIG.cache_dir,
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        print(f"âœ… åˆ›å»ºç›®å½•: {directory}")


def print_config():
    """æ‰“å°æ‰€æœ‰é…ç½®ä¿¡æ¯"""
    print("=" * 50)
    print("T5æ¡†æ¶é…ç½®ä¿¡æ¯")
    print("=" * 50)
    
    print("\nğŸ—ï¸ æ¨¡å‹é…ç½®:")
    print(f"  è¯æ±‡è¡¨å¤§å°: {T5_CONFIG.vocab_size:,}")
    print(f"  æ¨¡å‹ç»´åº¦: {T5_CONFIG.d_model}")
    print(f"  å±‚æ•°: {T5_CONFIG.num_layers}")
    print(f"  æ³¨æ„åŠ›å¤´æ•°: {T5_CONFIG.num_heads}")
    print(f"  æœ€å¤§åºåˆ—é•¿åº¦: {T5_CONFIG.max_length}")
    
    print("\nğŸš€ è®­ç»ƒé…ç½®:")
    print(f"  æ‰¹æ¬¡å¤§å°: {TRAINING_CONFIG.batch_size}")
    print(f"  å­¦ä¹ ç‡: {TRAINING_CONFIG.learning_rate}")
    print(f"  è®­ç»ƒè½®æ•°: {TRAINING_CONFIG.num_epochs}")
    print(f"  æœ€å¤§æ ·æœ¬æ•°: {TRAINING_CONFIG.max_samples}")
    print(f"  è®¾å¤‡: {get_device()}")
    
    print("\nğŸ“Š æ•°æ®é…ç½®:")
    print(f"  æ•°æ®é›†: {TRAINING_CONFIG.dataset_name}")
    print(f"  Tokenizer: {DATA_CONFIG.tokenizer_name}")
    print(f"  æºåºåˆ—é•¿åº¦: {DATA_CONFIG.max_source_length}")
    print(f"  ç›®æ ‡åºåˆ—é•¿åº¦: {DATA_CONFIG.max_target_length}")
    
    print("=" * 50)


if __name__ == "__main__":
    # æµ‹è¯•é…ç½®
    print_config()
    
    # æµ‹è¯•è®¾å¤‡æ£€æµ‹
    device = get_device()
    print(f"\næ£€æµ‹åˆ°çš„è®¾å¤‡: {device}")
    
    # æµ‹è¯•æ—¥å¿—
    logger = setup_logging()
    logger.info("T5é…ç½®æ¨¡å—æµ‹è¯•æˆåŠŸï¼")
