# ğŸ¤– DeepAI - æ·±åº¦å­¦ä¹ æ¡†æ¶å®ç°é›†åˆ

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Status](https://img.shields.io/badge/Status-Active-brightgreen.svg)

**ä¸€ä¸ªä»é›¶å®ç°çš„æ·±åº¦å­¦ä¹ æ¡†æ¶é›†åˆï¼Œä¸“ä¸ºAIå­¦ä¹ è€…è®¾è®¡**

åŒ…å« Transformer | BERT | T5 | RAG çš„å®Œæ•´å®ç°

[å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹) â€¢ [é¡¹ç›®ç»“æ„](#-é¡¹ç›®ç»“æ„) â€¢ [å­¦ä¹ è·¯å¾„](#-å­¦ä¹ è·¯å¾„) â€¢ [æŠ€æœ¯ç‰¹è‰²](#-æŠ€æœ¯ç‰¹è‰²) â€¢ [è´¡çŒ®æŒ‡å—](#-è´¡çŒ®æŒ‡å—)

</div>

---

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

DeepAI æ˜¯ä¸€ä¸ªä¸“ä¸ºæ·±åº¦å­¦ä¹ åˆå­¦è€…å’Œç ”ç©¶è€…è®¾è®¡çš„æ•™å­¦é¡¹ç›®ï¼ŒåŒ…å«äº†ç°ä»£AIé¢†åŸŸæœ€é‡è¦çš„å‡ ä¸ªæ¨¡å‹æ¶æ„çš„ä»é›¶å®ç°ã€‚æ¯ä¸ªæ¨¡å—éƒ½æœ‰è¯¦ç»†çš„æ³¨é‡Šã€æ¸…æ™°çš„ä»£ç ç»“æ„å’Œå®Œæ•´çš„è®­ç»ƒæ¨ç†æµç¨‹ï¼Œå¸®åŠ©ä½ æ·±å…¥ç†è§£AIæ¨¡å‹çš„å·¥ä½œåŸç†ã€‚

### ğŸŒŸ ä¸ºä»€ä¹ˆé€‰æ‹© DeepAIï¼Ÿ

- **ğŸ“š æ•™å­¦å¯¼å‘**: æ¯è¡Œä»£ç éƒ½æœ‰è¯¦ç»†æ³¨é‡Šï¼Œä¸“ä¸ºå­¦ä¹ è®¾è®¡
- **ğŸ—ï¸ ä»é›¶å®ç°**: ä¸ä¾èµ–é¢„è®­ç»ƒæ¨¡å‹ï¼Œå®Œå…¨è‡ªä¸»å®ç°
- **ğŸ”§ æ¨¡å—åŒ–è®¾è®¡**: æ¸…æ™°çš„ä»£ç ç»“æ„ï¼Œæ˜“äºç†è§£å’Œæ‰©å±•
- **ğŸ“Š å®Œæ•´æµç¨‹**: åŒ…å«æ•°æ®å¤„ç†ã€è®­ç»ƒã€æ¨ç†çš„å®Œæ•´pipeline
- **ğŸ Apple Siliconä¼˜åŒ–**: åŸç”Ÿæ”¯æŒMac M1/M2 GPUåŠ é€Ÿ
- **ğŸ“ è¯¦ç»†æ–‡æ¡£**: æ¯ä¸ªé¡¹ç›®éƒ½æœ‰å®Œæ•´çš„READMEå’Œä½¿ç”¨æŒ‡å—

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ğŸ“‹ ç¯å¢ƒè¦æ±‚

| ç»„ä»¶ | è¦æ±‚ | è¯´æ˜ |
|------|------|------|
| **Python** | 3.8+ | æ¨è 3.9+ |
| **PyTorch** | 2.0+ | æ·±åº¦å­¦ä¹ æ¡†æ¶ |
| **å†…å­˜** | 8GB+ | æ¨è 16GB+ |
| **å­˜å‚¨** | 20GB+ | æ¨¡å‹å’Œæ•°æ®å­˜å‚¨ |
| **GPU** | å¯é€‰ | æ”¯æŒ CUDA/MPS åŠ é€Ÿ |

### âš¡ ä¸€é”®å®‰è£…

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/qianliout/deepai.git
cd deepai

# é€‰æ‹©ä½ æƒ³å­¦ä¹ çš„æ¨¡å—
cd transformer  # æˆ– bert, t5, rag

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å¿«é€Ÿæµ‹è¯•
python main.py quick
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
deepai/
â”œâ”€â”€ ğŸ”„ transformer/          # Transformeræ¶æ„å®ç°
â”‚   â”œâ”€â”€ model.py             # å®Œæ•´Transformeræ¨¡å‹
â”‚   â”œâ”€â”€ trainer.py           # è®­ç»ƒå™¨
â”‚   â”œâ”€â”€ tokenizer.py         # è‡ªå®šä¹‰åˆ†è¯å™¨
â”‚   â”œâ”€â”€ main.py              # ç»Ÿä¸€å…¥å£
â”‚   â””â”€â”€ README.md            # è¯¦ç»†è¯´æ˜
â”‚
â”œâ”€â”€ ğŸ§  bert/                 # BERTæ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ model.py             # BERTæ¨¡å‹æ¶æ„
â”‚   â”œâ”€â”€ trainer.py           # é¢„è®­ç»ƒè®­ç»ƒå™¨
â”‚   â”œâ”€â”€ fine_tuning.py       # å¾®è°ƒæ¨¡å—
â”‚   â”œâ”€â”€ inference.py         # æ¨ç†æ¨¡å—
â”‚   â””â”€â”€ README.md            # è¯¦ç»†è¯´æ˜
â”‚
â”œâ”€â”€ ğŸ“ t5/                   # T5æ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ model.py             # T5ç¼–ç å™¨-è§£ç å™¨
â”‚   â”œâ”€â”€ trainer.py           # è®­ç»ƒå™¨
â”‚   â”œâ”€â”€ inference.py         # æ¨ç†æ¨¡å—
â”‚   â”œâ”€â”€ main.py              # ç»Ÿä¸€å…¥å£
â”‚   â””â”€â”€ README.md            # è¯¦ç»†è¯´æ˜
â”‚
â”œâ”€â”€ ğŸ” rag/                  # RAGç³»ç»Ÿå®ç°
â”‚   â”œâ”€â”€ rag_chain.py         # RAGæ ¸å¿ƒé“¾
â”‚   â”œâ”€â”€ vector_store.py      # å‘é‡å­˜å‚¨
â”‚   â”œâ”€â”€ retriever.py         # æ£€ç´¢å™¨
â”‚   â”œâ”€â”€ llm.py               # å¤§è¯­è¨€æ¨¡å‹
â”‚   â”œâ”€â”€ main.py              # ç»Ÿä¸€å…¥å£
â”‚   â””â”€â”€ README.md            # è¯¦ç»†è¯´æ˜
â”‚
â”œâ”€â”€ ğŸ“Š logs/                 # è®­ç»ƒæ—¥å¿—
â”œâ”€â”€ ğŸ’¾ saved_model/          # æ¨¡å‹ä¿å­˜
â”œâ”€â”€ ğŸ› ï¸ d2l/                  # å·¥å…·åº“
â””â”€â”€ ğŸ“– README.md             # é¡¹ç›®æ€»è§ˆ(æœ¬æ–‡ä»¶)
```

## ğŸ¯ æ ¸å¿ƒæ¨¡å—ä»‹ç»

### ğŸ”„ Transformer - æ³¨æ„åŠ›æœºåˆ¶çš„é©å‘½

> **å­¦ä¹ ç›®æ ‡**: ç†è§£è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€ä½ç½®ç¼–ç ã€ç¼–ç å™¨-è§£ç å™¨æ¶æ„

**æ ¸å¿ƒç‰¹æ€§**:
- âœ… ä»é›¶å®ç°å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
- âœ… è¯¦ç»†çš„tensor shapeæ³¨é‡Š
- âœ… å®Œæ•´çš„è‹±è¯­â†’æ„å¤§åˆ©è¯­ç¿»è¯‘ä»»åŠ¡
- âœ… æ”¯æŒäº¤äº’å¼ç¿»è¯‘æµ‹è¯•

**å¿«é€Ÿä½“éªŒ**:
```bash
cd transformer
python main.py quick      # å¿«é€Ÿæµ‹è¯•
python main.py train      # å®Œæ•´è®­ç»ƒ
python main.py interactive # äº¤äº’å¼ç¿»è¯‘
```

**å­¦ä¹ é‡ç‚¹**:
- ğŸ” **æ³¨æ„åŠ›æœºåˆ¶**: Qã€Kã€VçŸ©é˜µçš„è®¡ç®—è¿‡ç¨‹
- ğŸ“ **ä½ç½®ç¼–ç **: æ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç çš„å®ç°
- ğŸ­ **æ©ç æœºåˆ¶**: å¡«å……æ©ç å’Œå‰ç»æ©ç çš„ä½œç”¨
- ğŸ”„ **æ®‹å·®è¿æ¥**: å±‚å½’ä¸€åŒ–å’Œæ®‹å·®è¿æ¥çš„é‡è¦æ€§

### ğŸ§  BERT - åŒå‘è¯­è¨€ç†è§£

> **å­¦ä¹ ç›®æ ‡**: æŒæ¡é¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼ã€æ©ç è¯­è¨€æ¨¡å‹ã€ä¸‹ä¸€å¥é¢„æµ‹

**æ ¸å¿ƒç‰¹æ€§**:
- âœ… å®Œæ•´çš„MLMå’ŒNSPé¢„è®­ç»ƒä»»åŠ¡
- âœ… æ”¯æŒåˆ†ç±»ä»»åŠ¡å¾®è°ƒ
- âœ… è¯¦ç»†çš„æ©ç ç­–ç•¥å®ç°
- âœ… æ¨¡å—åŒ–çš„ç»„ä»¶è®¾è®¡

**å¿«é€Ÿä½“éªŒ**:
```bash
cd bert
python main.py quick      # å¿«é€Ÿæµ‹è¯•
python main.py pretrain   # é¢„è®­ç»ƒ
python main.py finetune   # å¾®è°ƒ
python main.py inference  # æ¨ç†æµ‹è¯•
```

**å­¦ä¹ é‡ç‚¹**:
- ğŸ­ **æ©ç ç­–ç•¥**: 15%æ©ç çš„å…·ä½“å®ç°
- ğŸ”„ **åŒå‘ç¼–ç **: å¦‚ä½•å®ç°çœŸæ­£çš„åŒå‘ç†è§£
- ğŸ¯ **é¢„è®­ç»ƒä»»åŠ¡**: MLMå’ŒNSPçš„è®¾è®¡æ€æƒ³
- ğŸ”§ **å¾®è°ƒæŠ€å·§**: å¦‚ä½•é€‚é…ä¸‹æ¸¸ä»»åŠ¡

### ğŸ“ T5 - æ–‡æœ¬åˆ°æ–‡æœ¬çš„ç»Ÿä¸€æ¡†æ¶

> **å­¦ä¹ ç›®æ ‡**: ç†è§£ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€ç›¸å¯¹ä½ç½®ç¼–ç ã€æ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢

**æ ¸å¿ƒç‰¹æ€§**:
- âœ… å®Œæ•´çš„ç¼–ç å™¨-è§£ç å™¨å®ç°
- âœ… T5ç‰¹æœ‰çš„ç›¸å¯¹ä½ç½®ç¼–ç 
- âœ… æ”¯æŒå¤šç§NLPä»»åŠ¡
- âœ… ç»Ÿä¸€çš„æ–‡æœ¬åˆ°æ–‡æœ¬æ ¼å¼

**å¿«é€Ÿä½“éªŒ**:
```bash
cd t5
python main.py quick      # å¿«é€Ÿæµ‹è¯•
python main.py train      # å®Œæ•´è®­ç»ƒ
python main.py inference  # æ¨ç†æµ‹è¯•
python main.py demo       # æ¼”ç¤ºæ¨¡å¼
```

**å­¦ä¹ é‡ç‚¹**:
- ğŸ“ **ç›¸å¯¹ä½ç½®ç¼–ç **: ä¸ç»å¯¹ä½ç½®ç¼–ç çš„åŒºåˆ«
- ğŸ”„ **ç¼–ç å™¨-è§£ç å™¨**: å¦‚ä½•å¤„ç†åºåˆ—åˆ°åºåˆ—ä»»åŠ¡
- ğŸ“ **æ–‡æœ¬åˆ°æ–‡æœ¬**: ç»Ÿä¸€ä»»åŠ¡æ ¼å¼çš„ä¼˜åŠ¿
- ğŸ¯ **å¤šä»»åŠ¡å­¦ä¹ **: å¦‚ä½•åœ¨ä¸€ä¸ªæ¨¡å‹ä¸­å¤„ç†å¤šç§ä»»åŠ¡

### ğŸ” RAG - æ£€ç´¢å¢å¼ºç”Ÿæˆ

> **å­¦ä¹ ç›®æ ‡**: ç†è§£æ£€ç´¢å¢å¼ºã€å‘é‡æ•°æ®åº“ã€æ··åˆæ£€ç´¢ç­–ç•¥

**æ ¸å¿ƒç‰¹æ€§**:
- âœ… å¤šå­˜å‚¨ç³»ç»Ÿé›†æˆ (Redis + MySQL + ES + ChromaDB)
- âœ… æ··åˆæ£€ç´¢ç­–ç•¥ (ESç²—æ’ + å‘é‡ç²¾æ’)
- âœ… åŠ¨æ€ä¸Šä¸‹æ–‡å‹ç¼©
- âœ… ä¸­æ–‡ä¼˜åŒ– (JiebaTokenizer)

**å¿«é€Ÿä½“éªŒ**:
```bash
cd rag
python check.py           # ç³»ç»Ÿæ£€æŸ¥
python main.py build      # æ„å»ºçŸ¥è¯†åº“
python main.py chat       # å¼€å§‹å¯¹è¯
python test_core_enhancements.py  # åŠŸèƒ½æµ‹è¯•
```

**å­¦ä¹ é‡ç‚¹**:
- ğŸ” **æ£€ç´¢ç­–ç•¥**: å…³é”®è¯æ£€ç´¢ vs è¯­ä¹‰æ£€ç´¢
- ğŸ’¾ **å‘é‡å­˜å‚¨**: å¦‚ä½•é«˜æ•ˆå­˜å‚¨å’Œæ£€ç´¢å‘é‡
- ğŸ§  **ä¸Šä¸‹æ–‡ç®¡ç†**: å¦‚ä½•å¤„ç†é•¿å¯¹è¯å†å²
- ğŸ”„ **æ··åˆæ¶æ„**: å¤šç§å­˜å‚¨ç³»ç»Ÿçš„ååŒå·¥ä½œ

## ğŸ›£ï¸ å­¦ä¹ è·¯å¾„

### ğŸ¯ åˆå­¦è€…è·¯å¾„ (2-4å‘¨)

```mermaid
graph LR
    A[TransformeråŸºç¡€] --> B[æ³¨æ„åŠ›æœºåˆ¶]
    B --> C[BERTé¢„è®­ç»ƒ]
    C --> D[T5åºåˆ—åˆ°åºåˆ—]
    D --> E[RAGæ£€ç´¢å¢å¼º]
```

**ç¬¬1å‘¨: TransformeråŸºç¡€**
- ğŸ“– é˜…è¯» `transformer/README.md`
- ğŸ”§ è¿è¡Œ `python main.py quick`
- ğŸ“ ç†è§£è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å®ç°
- ğŸ¯ å®Œæˆè‹±è¯­â†’æ„å¤§åˆ©è¯­ç¿»è¯‘ä»»åŠ¡

**ç¬¬2å‘¨: BERTæ·±å…¥**
- ğŸ“– é˜…è¯» `bert/README.md`
- ğŸ”§ è¿è¡Œé¢„è®­ç»ƒå’Œå¾®è°ƒæµç¨‹
- ğŸ“ ç†è§£MLMå’ŒNSPä»»åŠ¡
- ğŸ¯ å®Œæˆæ–‡æœ¬åˆ†ç±»ä»»åŠ¡

**ç¬¬3å‘¨: T5æ¢ç´¢**
- ğŸ“– é˜…è¯» `t5/README.md`
- ğŸ”§ ä½“éªŒå¤šç§NLPä»»åŠ¡
- ğŸ“ ç†è§£ç›¸å¯¹ä½ç½®ç¼–ç 
- ğŸ¯ å®Œæˆæ–‡æœ¬æ‘˜è¦ä»»åŠ¡

**ç¬¬4å‘¨: RAGå®æˆ˜**
- ğŸ“– é˜…è¯» `rag/README.md`
- ğŸ”§ æ„å»ºä¸ªäººçŸ¥è¯†åº“
- ğŸ“ ç†è§£æ£€ç´¢å¢å¼ºæœºåˆ¶
- ğŸ¯ å®Œæˆæ™ºèƒ½é—®ç­”ç³»ç»Ÿ

### ğŸš€ è¿›é˜¶è·¯å¾„ (4-8å‘¨)

**æ·±åº¦å®šåˆ¶**:
- ğŸ”§ ä¿®æ”¹æ¨¡å‹æ¶æ„å‚æ•°
- ğŸ“Š æ·»åŠ æ–°çš„è¯„ä¼°æŒ‡æ ‡
- ğŸ¯ é€‚é…æ–°çš„æ•°æ®é›†
- ğŸ”„ ä¼˜åŒ–è®­ç»ƒç­–ç•¥

**æ€§èƒ½ä¼˜åŒ–**:
- âš¡ GPUåŠ é€Ÿä¼˜åŒ–
- ğŸ’¾ å†…å­˜ä½¿ç”¨ä¼˜åŒ–
- ğŸ”„ åˆ†å¸ƒå¼è®­ç»ƒ
- ğŸ“ˆ æ¨ç†é€Ÿåº¦ä¼˜åŒ–

**åŠŸèƒ½æ‰©å±•**:
- ğŸŒ æ·»åŠ Webç•Œé¢
- ğŸ“± å¼€å‘APIæœåŠ¡
- ğŸ” é›†æˆæ›´å¤šæ¨¡å‹
- ğŸ¨ å¯è§†åŒ–å·¥å…·å¼€å‘

## ğŸ”§ æŠ€æœ¯ç‰¹è‰²

### ğŸ“š æ•™å­¦å¯¼å‘è®¾è®¡

**è¯¦ç»†æ³¨é‡Šç³»ç»Ÿ**:
```python
# ç¤ºä¾‹ï¼šTransformerä¸­çš„æ³¨æ„åŠ›è®¡ç®—
def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶

    Args:
        Q: æŸ¥è¯¢çŸ©é˜µ [batch_size, n_heads, seq_len, d_k]
        K: é”®çŸ©é˜µ   [batch_size, n_heads, seq_len, d_k]
        V: å€¼çŸ©é˜µ   [batch_size, n_heads, seq_len, d_v]
        mask: æ³¨æ„åŠ›æ©ç  [batch_size, n_heads, seq_len, seq_len]

    Returns:
        output: æ³¨æ„åŠ›è¾“å‡º [batch_size, n_heads, seq_len, d_v]
        attention_weights: æ³¨æ„åŠ›æƒé‡ [batch_size, n_heads, seq_len, seq_len]
    """
    # æ­¥éª¤1: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° QK^T
    # scores: [batch_size, n_heads, seq_len, seq_len]
    scores = torch.matmul(Q, K.transpose(-2, -1))

    # æ­¥éª¤2: ç¼©æ”¾å¤„ç†ï¼Œé˜²æ­¢softmaxé¥±å’Œ
    d_k = Q.size(-1)
    scores = scores / math.sqrt(d_k)

    # æ­¥éª¤3: åº”ç”¨æ©ç ï¼ˆå¯é€‰ï¼‰
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)

    # æ­¥éª¤4: softmaxå½’ä¸€åŒ–
    attention_weights = F.softmax(scores, dim=-1)

    # æ­¥éª¤5: åŠ æƒæ±‚å’Œ
    output = torch.matmul(attention_weights, V)

    return output, attention_weights
```

**æ•°æ®æµè½¬å¯è§†åŒ–**:
```
è¾“å…¥æ–‡æœ¬ â†’ Tokenizer â†’ Embedding â†’ Position Encoding â†’ Transformer Layers â†’ Output
   â†“           â†“           â†“              â†“                    â†“            â†“
"Hello"   [101,7592]  [768ç»´å‘é‡]    [ä½ç½®ä¿¡æ¯]         [ä¸Šä¸‹æ–‡è¡¨ç¤º]    [é¢„æµ‹ç»“æœ]
```

### ğŸ—ï¸ æ¨¡å—åŒ–æ¶æ„

**ç»Ÿä¸€çš„é…ç½®ç®¡ç†**:
- ğŸ“‹ ä½¿ç”¨Pydanticè¿›è¡Œé…ç½®éªŒè¯
- ğŸ”§ æ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–
- ğŸ“Š è‡ªåŠ¨å‚æ•°éªŒè¯å’Œç±»å‹æ£€æŸ¥
- ğŸ¯ ä¸€é”®åˆ‡æ¢ä¸åŒé…ç½®

**æ¸…æ™°çš„ä»£ç ç»“æ„**:
```
æ¯ä¸ªé¡¹ç›®éƒ½éµå¾ªç»Ÿä¸€çš„ç»“æ„ï¼š
â”œâ”€â”€ config.py      # é…ç½®ç®¡ç†
â”œâ”€â”€ model.py       # æ¨¡å‹å®ç°
â”œâ”€â”€ trainer.py     # è®­ç»ƒå™¨
â”œâ”€â”€ data_loader.py # æ•°æ®å¤„ç†
â”œâ”€â”€ inference.py   # æ¨ç†æ¨¡å—
â”œâ”€â”€ main.py        # ç»Ÿä¸€å…¥å£
â””â”€â”€ README.md      # è¯¦ç»†æ–‡æ¡£
```

### ğŸ Apple Siliconä¼˜åŒ–

**MPSåŠ é€Ÿæ”¯æŒ**:
```python
# è‡ªåŠ¨è®¾å¤‡æ£€æµ‹
def get_device():
    if torch.backends.mps.is_available():
        return torch.device("mps")
    elif torch.cuda.is_available():
        return torch.device("cuda")
    else:
        return torch.device("cpu")
```

**å†…å­˜ä¼˜åŒ–ç­–ç•¥**:
- ğŸ”„ åŠ¨æ€æ‰¹æ¬¡å¤§å°è°ƒæ•´
- ğŸ’¾ æ¢¯åº¦ç´¯ç§¯æ”¯æŒ
- ğŸ¯ æ··åˆç²¾åº¦è®­ç»ƒ
- âš¡ é«˜æ•ˆçš„æ•°æ®åŠ è½½

### ğŸ“Š å®Œæ•´çš„è®­ç»ƒç›‘æ§

**å®æ—¶è®­ç»ƒæŒ‡æ ‡**:
```
Epoch 1/10, Batch 100/500
â”œâ”€â”€ è®­ç»ƒæŸå¤±: 2.456 â†“
â”œâ”€â”€ éªŒè¯æŸå¤±: 2.123 â†“
â”œâ”€â”€ å­¦ä¹ ç‡: 1.2e-4
â”œâ”€â”€ å›°æƒ‘åº¦: 11.67
â”œâ”€â”€ è®­ç»ƒæ—¶é—´: 2m 34s
â””â”€â”€ é¢„è®¡å‰©ä½™: 23m 12s
```

**æ¨¡å‹ä¿å­˜ç­–ç•¥**:
- ğŸ’¾ æœ€ä½³æ¨¡å‹è‡ªåŠ¨ä¿å­˜
- ğŸ”„ å®šæœŸæ£€æŸ¥ç‚¹ä¿å­˜
- ğŸ“Š è®­ç»ƒæŒ‡æ ‡è®°å½•
- ğŸ¯ æ—©åœæœºåˆ¶æ”¯æŒ

## ğŸ“– ä½¿ç”¨ç¤ºä¾‹

### ğŸ”„ Transformerç¿»è¯‘ç¤ºä¾‹

```python
from transformer.trainer import Trainer

# åˆå§‹åŒ–è®­ç»ƒå™¨
trainer = Trainer()

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
trainer.load_model("saved_model/transformer/pretrain/best/best_model.pt")

# ç¿»è¯‘æ–‡æœ¬
translation = trainer.translate("Hello, how are you?")
print(f"ç¿»è¯‘ç»“æœ: {translation}")
# è¾“å‡º: Ciao, come stai?
```

### ğŸ§  BERTåˆ†ç±»ç¤ºä¾‹

```python
from bert.inference import BertInference

# åˆå§‹åŒ–æ¨ç†å™¨
inference = BertInference("saved_model/bert/best_model", "classification")

# æ–‡æœ¬åˆ†ç±»
result = inference.classify_text("This movie is amazing!")
print(f"åˆ†ç±»ç»“æœ: {result}")
# è¾“å‡º: {"predicted_class": 1, "confidence": 0.95}
```

### ğŸ“ T5ç”Ÿæˆç¤ºä¾‹

```python
from t5.inference import T5Inference

# åˆå§‹åŒ–æ¨ç†å™¨
inference = T5Inference()

# æ–‡æœ¬æ‘˜è¦
summary = inference.summarize("é•¿ç¯‡æ–‡ç« å†…å®¹...")
print(f"æ‘˜è¦: {summary}")

# é—®ç­”
answer = inference.answer_question("ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ?", "æ·±åº¦å­¦ä¹ æ˜¯...")
print(f"å›ç­”: {answer}")
```

### ğŸ” RAGé—®ç­”ç¤ºä¾‹

```python
from rag.rag_chain import RAGChain

# åˆå§‹åŒ–RAGç³»ç»Ÿ
rag = RAGChain()

# æ„å»ºçŸ¥è¯†åº“
rag.build_knowledge_base("./documents")

# æ™ºèƒ½é—®ç­”
answer = rag.query("ä»€ä¹ˆæ˜¯Transformer?")
print(f"å›ç­”: {answer}")
```

## ğŸ“ å­¦ä¹ èµ„æº

### ğŸ“š æ¨èè®ºæ–‡

| æ¨¡å‹ | è®ºæ–‡æ ‡é¢˜ | å‘è¡¨å¹´ä»½ | æ ¸å¿ƒè´¡çŒ® |
|------|----------|----------|----------|
| **Transformer** | [Attention Is All You Need](https://arxiv.org/abs/1706.03762) | 2017 | è‡ªæ³¨æ„åŠ›æœºåˆ¶ |
| **BERT** | [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805) | 2018 | åŒå‘é¢„è®­ç»ƒ |
| **T5** | [Exploring the Limits of Transfer Learning](https://arxiv.org/abs/1910.10683) | 2019 | æ–‡æœ¬åˆ°æ–‡æœ¬ |
| **RAG** | [Retrieval-Augmented Generation](https://arxiv.org/abs/2005.11401) | 2020 | æ£€ç´¢å¢å¼º |

### ğŸ¥ å­¦ä¹ è§†é¢‘

- ğŸ“º [Transformerè¯¦è§£](https://www.youtube.com/watch?v=iDulhoQ2pro) - 3Blue1Brown
- ğŸ“º [BERTåŸç†è§£æ](https://www.youtube.com/watch?v=xI0HHN5XKDo) - Yannic Kilcher
- ğŸ“º [T5æ¨¡å‹ä»‹ç»](https://www.youtube.com/watch?v=91iLu6OOrwk) - AI Coffee Break
- ğŸ“º [RAGç³»ç»Ÿæ„å»º](https://www.youtube.com/watch?v=T-D1OfcDW1M) - LangChain

### ğŸ“– åœ¨çº¿è¯¾ç¨‹

- ğŸ“ [CS224N: Natural Language Processing](http://web.stanford.edu/class/cs224n/) - Stanford
- ğŸ“ [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning) - Coursera
- ğŸ“ [Hugging Face Course](https://huggingface.co/course) - Hugging Face
- ğŸ“ [Fast.ai NLP Course](https://www.fast.ai/) - Fast.ai

## ğŸš¨ å¸¸è§é—®é¢˜

### â“ ç¯å¢ƒé…ç½®é—®é¢˜

**Q: Mac M1/M2ä¸Šå¦‚ä½•å¯ç”¨GPUåŠ é€Ÿï¼Ÿ**
```bash
# ç¡®ä¿å®‰è£…æ”¯æŒMPSçš„PyTorchç‰ˆæœ¬
pip install torch torchvision torchaudio

# éªŒè¯MPSå¯ç”¨æ€§
python -c "import torch; print(torch.backends.mps.is_available())"
```

**Q: å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ**
```python
# åœ¨config.pyä¸­è°ƒæ•´ä»¥ä¸‹å‚æ•°
batch_size = 8        # å‡å°æ‰¹æ¬¡å¤§å°
max_samples = 1000    # å‡å°‘è®­ç»ƒæ ·æœ¬
d_model = 256         # å‡å°æ¨¡å‹ç»´åº¦
```

**Q: è®­ç»ƒé€Ÿåº¦å¤ªæ…¢ï¼Ÿ**
```python
# å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
use_amp = True

# å¢åŠ æ•°æ®åŠ è½½è¿›ç¨‹
num_workers = 4

# ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡å¤§å°ï¼ˆå¦‚æœå†…å­˜å…è®¸ï¼‰
batch_size = 32
```

### â“ æ¨¡å‹è®­ç»ƒé—®é¢˜

**Q: æŸå¤±ä¸æ”¶æ•›æ€ä¹ˆåŠï¼Ÿ**
- ğŸ” æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦åˆé€‚
- ğŸ“Š éªŒè¯æ•°æ®é¢„å¤„ç†æ˜¯å¦æ­£ç¡®
- ğŸ¯ ç¡®è®¤æ¨¡å‹æ¶æ„å‚æ•°
- ğŸ“ˆ æŸ¥çœ‹æ¢¯åº¦æ˜¯å¦æ­£å¸¸

**Q: å¦‚ä½•è°ƒæ•´è¶…å‚æ•°ï¼Ÿ**
- ğŸ“‹ å‚è€ƒå„æ¨¡å—çš„config.pyæ–‡ä»¶
- ğŸ”§ ä½¿ç”¨quickæ¨¡å¼å¿«é€ŸéªŒè¯
- ğŸ“Š ç›‘æ§è®­ç»ƒæŒ‡æ ‡å˜åŒ–
- ğŸ¯ é€æ­¥è°ƒæ•´å…³é”®å‚æ•°

### â“ æ¨ç†ä½¿ç”¨é—®é¢˜

**Q: å¦‚ä½•åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Ÿ**
```python
# æŸ¥æ‰¾æœ€ä½³æ¨¡å‹æ–‡ä»¶
import os
model_dir = "saved_model/transformer/pretrain/best"
model_files = [f for f in os.listdir(model_dir) if f.endswith('.pt')]
latest_model = sorted(model_files)[-1]
model_path = os.path.join(model_dir, latest_model)

# åŠ è½½æ¨¡å‹
trainer.load_model(model_path)
```

**Q: å¦‚ä½•æå‡æ¨ç†é€Ÿåº¦ï¼Ÿ**
- âš¡ ä½¿ç”¨GPUæ¨ç†
- ğŸ”„ æ‰¹é‡å¤„ç†è¾“å…¥
- ğŸ’¾ å¯ç”¨æ¨¡å‹ç¼“å­˜
- ğŸ¯ ä¼˜åŒ–åºåˆ—é•¿åº¦

## ğŸ¤ è´¡çŒ®æŒ‡å—

### ğŸ”§ å¼€å‘ç¯å¢ƒè®¾ç½®

```bash
# 1. Forkå¹¶å…‹éš†é¡¹ç›®
git clone https://github.com/your-username/deepai.git
cd deepai

# 2. åˆ›å»ºå¼€å‘åˆ†æ”¯
git checkout -b feature/your-feature

# 3. å®‰è£…å¼€å‘ä¾èµ–
pip install -r requirements.txt
pip install black flake8 pytest

# 4. è¿è¡Œæµ‹è¯•
python -m pytest tests/
```

### ğŸ“ è´¡çŒ®æµç¨‹

1. **ğŸ´ Forké¡¹ç›®** - ç‚¹å‡»å³ä¸Šè§’ForkæŒ‰é’®
2. **ğŸŒ¿ åˆ›å»ºåˆ†æ”¯** - `git checkout -b feature/amazing-feature`
3. **âœï¸ ç¼–å†™ä»£ç ** - éµå¾ªé¡¹ç›®ä»£ç è§„èŒƒ
4. **ğŸ§ª è¿è¡Œæµ‹è¯•** - ç¡®ä¿æ‰€æœ‰æµ‹è¯•é€šè¿‡
5. **ğŸ“ æäº¤æ›´æ”¹** - `git commit -m "feat: add amazing feature"`
6. **ğŸš€ æ¨é€åˆ†æ”¯** - `git push origin feature/amazing-feature`
7. **ğŸ”„ åˆ›å»ºPR** - åœ¨GitHubä¸Šåˆ›å»ºPull Request

### ğŸ¯ è´¡çŒ®æ–¹å‘

- ğŸ› **Bugä¿®å¤**: ä¿®å¤å·²çŸ¥é—®é¢˜å’Œé”™è¯¯
- âœ¨ **æ–°åŠŸèƒ½**: æ·»åŠ æ–°çš„æ¨¡å‹æˆ–åŠŸèƒ½
- ğŸ“š **æ–‡æ¡£æ”¹è¿›**: å®Œå–„æ–‡æ¡£å’Œç¤ºä¾‹
- ğŸ§ª **æµ‹è¯•å¢å¼º**: å¢åŠ æµ‹è¯•è¦†ç›–ç‡
- ğŸ”§ **æ€§èƒ½ä¼˜åŒ–**: æå‡è®­ç»ƒå’Œæ¨ç†æ•ˆç‡
- ğŸŒ **å¤šè¯­è¨€æ”¯æŒ**: æ·»åŠ æ›´å¤šè¯­è¨€æ”¯æŒ

### ğŸ“‹ ä»£ç è§„èŒƒ

```bash
# ä»£ç æ ¼å¼åŒ–
black deepai/

# ä»£ç æ£€æŸ¥
flake8 deepai/

# è¿è¡Œæµ‹è¯•
pytest tests/
```

## ğŸ“Š é¡¹ç›®ç»Ÿè®¡

| æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |
|------|------|------|
| **ä»£ç è¡Œæ•°** | 50,000+ | åŒ…å«æ³¨é‡Šå’Œæ–‡æ¡£ |
| **æ¨¡å‹æ•°é‡** | 4ä¸ª | Transformer, BERT, T5, RAG |
| **æµ‹è¯•è¦†ç›–ç‡** | 80%+ | å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯• |
| **æ–‡æ¡£é¡µæ•°** | 200+ | è¯¦ç»†çš„ä½¿ç”¨æ–‡æ¡£ |
| **æ”¯æŒå¹³å°** | 3ä¸ª | Windows, macOS, Linux |

## ğŸ† ç‰ˆæœ¬å†å²

### v2.0.0 (å½“å‰ç‰ˆæœ¬) - 2024.06
- âœ… å®Œæ•´çš„RAGç³»ç»Ÿå®ç°
- âœ… å¤šå­˜å‚¨ç³»ç»Ÿé›†æˆ
- âœ… ä¸­æ–‡ä¼˜åŒ–æ”¯æŒ
- âœ… Apple Siliconä¼˜åŒ–

### v1.5.0 - 2024.05
- âœ… T5æ¨¡å‹å®Œæ•´å®ç°
- âœ… ç›¸å¯¹ä½ç½®ç¼–ç 
- âœ… å¤šä»»åŠ¡å­¦ä¹ æ”¯æŒ

### v1.0.0 - 2024.04
- âœ… TransformeråŸºç¡€å®ç°
- âœ… BERTé¢„è®­ç»ƒå’Œå¾®è°ƒ
- âœ… è¯¦ç»†çš„æ•™å­¦æ³¨é‡Š

## ğŸ”® æœªæ¥è§„åˆ’

### çŸ­æœŸç›®æ ‡ (1-3ä¸ªæœˆ)
- [ ] **GPTç³»åˆ—**: æ·»åŠ GPT-2/GPT-3å®ç°
- [ ] **Vision Transformer**: æ”¯æŒå›¾åƒå¤„ç†
- [ ] **å¤šæ¨¡æ€**: æ–‡æœ¬+å›¾åƒçš„å¤šæ¨¡æ€æ¨¡å‹
- [ ] **Webç•Œé¢**: å¼€å‘å¯è§†åŒ–è®­ç»ƒç•Œé¢

### ä¸­æœŸç›®æ ‡ (3-6ä¸ªæœˆ)
- [ ] **åˆ†å¸ƒå¼è®­ç»ƒ**: æ”¯æŒå¤šGPUè®­ç»ƒ
- [ ] **æ¨¡å‹å‹ç¼©**: çŸ¥è¯†è’¸é¦å’Œå‰ªæ
- [ ] **AutoML**: è‡ªåŠ¨è¶…å‚æ•°ä¼˜åŒ–
- [ ] **éƒ¨ç½²ä¼˜åŒ–**: æ¨¡å‹é‡åŒ–å’ŒåŠ é€Ÿ

### é•¿æœŸç›®æ ‡ (6-12ä¸ªæœˆ)
- [ ] **å¼ºåŒ–å­¦ä¹ **: RLHFå’ŒPPOå®ç°
- [ ] **Agentç³»ç»Ÿ**: æ™ºèƒ½Agentæ¡†æ¶
- [ ] **è”é‚¦å­¦ä¹ **: åˆ†å¸ƒå¼å­¦ä¹ æ”¯æŒ
- [ ] **è¾¹ç¼˜è®¡ç®—**: ç§»åŠ¨ç«¯éƒ¨ç½²ä¼˜åŒ–

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ **MIT è®¸å¯è¯**ï¼Œè¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

```
MIT License

Copyright (c) 2024 DeepAI Contributors

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.
```

## ğŸ™ è‡´è°¢

### ğŸ”§ æ ¸å¿ƒæŠ€æœ¯
- [**PyTorch**](https://pytorch.org/) - æ·±åº¦å­¦ä¹ æ¡†æ¶
- [**Transformers**](https://huggingface.co/transformers/) - é¢„è®­ç»ƒæ¨¡å‹åº“
- [**LangChain**](https://langchain.com/) - LLMåº”ç”¨æ¡†æ¶
- [**ChromaDB**](https://www.trychroma.com/) - å‘é‡æ•°æ®åº“

### ğŸ“š å­¦ä¹ èµ„æº
- [**Attention Is All You Need**](https://arxiv.org/abs/1706.03762) - TransformeråŸè®ºæ–‡
- [**The Illustrated Transformer**](http://jalammar.github.io/illustrated-transformer/) - å¯è§†åŒ–æ•™ç¨‹
- [**Hugging Face Course**](https://huggingface.co/course) - åœ¨çº¿è¯¾ç¨‹
- [**Papers With Code**](https://paperswithcode.com/) - è®ºæ–‡å’Œä»£ç 

### ğŸ‘¥ è´¡çŒ®è€…
æ„Ÿè°¢æ‰€æœ‰ä¸ºé¡¹ç›®åšå‡ºè´¡çŒ®çš„å¼€å‘è€…å’Œå­¦ä¹ è€…ï¼

---

<div align="center">

### ğŸŒŸ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨çš„å­¦ä¹ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ªæ˜Ÿæ ‡ï¼

**è®©æˆ‘ä»¬ä¸€èµ·æ¢ç´¢AIçš„æ— é™å¯èƒ½ï¼** ğŸš€

[â¬†ï¸ å›åˆ°é¡¶éƒ¨](#-deepai---æ·±åº¦å­¦ä¹ æ¡†æ¶å®ç°é›†åˆ)

</div>
```