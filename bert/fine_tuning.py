"""
å¾®è°ƒæ¨¡å— - BERTåˆ†ç±»ä»»åŠ¡å¾®è°ƒ
æ”¯æŒä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æƒé‡ï¼Œè¿›è¡Œåˆ†ç±»ä»»åŠ¡å¾®è°ƒ
é‡ç‚¹å…³æ³¨å¾®è°ƒè¿‡ç¨‹çš„æ•°æ®æµè½¬ï¼ŒåŒ…å«è¯¦ç»†çš„shapeæ³¨é‡Š
"""

import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.optim.lr_scheduler import LinearLR
import os
import json
import logging
from typing import Dict, Any, Optional, Tuple, List
from tqdm import tqdm
import time
from pathlib import Path
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

from config import BERT_CONFIG, TRAINING_CONFIG, DATA_CONFIG, get_device, setup_logging
from model import BertForSequenceClassification
from data_loader import load_imdb_dataset, create_classification_dataloader
from transformers import AutoTokenizer

logger = logging.getLogger("BERT")


class BertFineTuner:
    """
    BERTå¾®è°ƒå™¨

    è´Ÿè´£å®Œæ•´çš„å¾®è°ƒæµç¨‹ï¼š
    1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    2. å‡†å¤‡åˆ†ç±»æ•°æ®
    3. å¾®è°ƒè®­ç»ƒ
    4. æ¨¡å‹è¯„ä¼°
    5. æ¨¡å‹ä¿å­˜
    """

    def __init__(self, pretrained_model_path: Optional[str] = None, num_labels: int = 2):
        """
        åˆå§‹åŒ–å¾®è°ƒå™¨

        Args:
            pretrained_model_path: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤è·¯å¾„
            num_labels: åˆ†ç±»æ ‡ç­¾æ•°é‡
        """
        # è®¾ç½®æ—¥å¿—
        setup_logging()

        # è®¾å¤‡é…ç½®
        self.device = get_device()
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

        # æ¨¡å‹é…ç½® - ä½¿ç”¨ç»Ÿä¸€çš„é…ç½®ç®¡ç†
        if pretrained_model_path is None:
            self.pretrained_model_path = Path(TRAINING_CONFIG.pretrained_model_path)
        else:
            self.pretrained_model_path = Path(pretrained_model_path)
        self.num_labels = num_labels

        # åˆ›å»ºå¾®è°ƒæ¨¡å‹ä¿å­˜ç›®å½• - ä½¿ç”¨ç»Ÿä¸€çš„é…ç½®ç®¡ç†
        self.fine_tuning_save_dir = Path(TRAINING_CONFIG.fine_tuning_save_dir)
        self.fine_tuning_save_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–tokenizer
        self.tokenizer = self._load_tokenizer()

        # åˆå§‹åŒ–æ¨¡å‹
        self.model = self._create_and_load_model()

        # åˆå§‹åŒ–æ•°æ®
        self.train_dataloader, self.val_dataloader = self._create_dataloaders()

        # åˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self.optimizer = self._create_optimizer()
        self.scheduler = self._create_scheduler()

        # è®­ç»ƒçŠ¶æ€
        self.global_step = 0
        self.epoch = 0
        self.best_accuracy = 0.0

        # è®­ç»ƒå†å²
        self.training_history = {
            "train_loss": [],
            "val_loss": [],
            "val_accuracy": [],
            "val_precision": [],
            "val_recall": [],
            "val_f1": [],
            "learning_rate": [],
            "epochs": [],
        }

        logger.info("å¾®è°ƒå™¨åˆå§‹åŒ–å®Œæˆ")

    def _load_tokenizer(self) -> AutoTokenizer:
        """åŠ è½½tokenizer"""
        logger.info(f"åŠ è½½tokenizer: {DATA_CONFIG.tokenizer_name}")
        tokenizer = AutoTokenizer.from_pretrained(
            DATA_CONFIG.tokenizer_name,
            cache_dir=TRAINING_CONFIG.cache_dir
        )

        # ç¡®ä¿æœ‰å¿…è¦çš„ç‰¹æ®Štoken
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        return tokenizer

    def _create_and_load_model(self) -> BertForSequenceClassification:
        """
        åˆ›å»ºå¹¶åŠ è½½é¢„è®­ç»ƒæ¨¡å‹

        Returns:
            BertForSequenceClassification: åˆ†ç±»æ¨¡å‹
        """
        logger.info("åˆ›å»ºåˆ†ç±»æ¨¡å‹...")

        # åˆ›å»ºåˆ†ç±»æ¨¡å‹
        model = BertForSequenceClassification(num_labels=self.num_labels)

        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        if self.pretrained_model_path.exists():
            logger.info(f"åŠ è½½é¢„è®­ç»ƒæƒé‡: {self.pretrained_model_path}")

            # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡
            pretrained_state_dict = torch.load(self.pretrained_model_path / "pytorch_model.bin", map_location=self.device)

            # è¿‡æ»¤æ‰åˆ†ç±»å¤´çš„æƒé‡ï¼ˆå› ä¸ºé¢„è®­ç»ƒæ¨¡å‹æ²¡æœ‰åˆ†ç±»å¤´ï¼‰
            model_state_dict = model.state_dict()
            filtered_state_dict = {}

            for key, value in pretrained_state_dict.items():
                if key in model_state_dict and model_state_dict[key].shape == value.shape:
                    filtered_state_dict[key] = value
                else:
                    logger.warning(f"è·³è¿‡æƒé‡: {key} (shapeä¸åŒ¹é…æˆ–ä¸å­˜åœ¨)")

            # åŠ è½½è¿‡æ»¤åçš„æƒé‡
            model.load_state_dict(filtered_state_dict, strict=False)
            logger.info(f"æˆåŠŸåŠ è½½ {len(filtered_state_dict)} ä¸ªé¢„è®­ç»ƒæƒé‡")
        else:
            logger.warning(f"é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.pretrained_model_path}")
            logger.warning("ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡")

        model.to(self.device)

        logger.info(f"åˆ†ç±»æ¨¡å‹å‚æ•°æ•°é‡: {model.count_parameters():,}")

        return model

    def _create_dataloaders(self) -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:
        """
        åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨

        Returns:
            (è®­ç»ƒæ•°æ®åŠ è½½å™¨, éªŒè¯æ•°æ®åŠ è½½å™¨)
        """
        logger.info("åˆ›å»ºåˆ†ç±»æ•°æ®åŠ è½½å™¨...")

        # åŠ è½½IMDBæ•°æ®é›†
        texts, labels = load_imdb_dataset(TRAINING_CONFIG.max_samples)

        # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†
        split_idx = int(len(texts) * 0.8)
        train_texts, val_texts = texts[:split_idx], texts[split_idx:]
        train_labels, val_labels = labels[:split_idx], labels[split_idx:]

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataloader = create_classification_dataloader(train_texts, train_labels, self.tokenizer, shuffle=True)
        val_dataloader = create_classification_dataloader(val_texts, val_labels, self.tokenizer, shuffle=False)

        logger.info(f"è®­ç»ƒæ ·æœ¬: {len(train_texts)}, éªŒè¯æ ·æœ¬: {len(val_texts)}")
        logger.info(f"è®­ç»ƒæ‰¹æ¬¡: {len(train_dataloader)}, éªŒè¯æ‰¹æ¬¡: {len(val_dataloader)}")

        return train_dataloader, val_dataloader

    def _create_optimizer(self) -> torch.optim.Optimizer:
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        logger.info("åˆ›å»ºå¾®è°ƒä¼˜åŒ–å™¨...")

        # ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡è¿›è¡Œå¾®è°ƒ
        fine_tune_lr = TRAINING_CONFIG.learning_rate * 0.1  # é€šå¸¸å¾®è°ƒä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡

        # åˆ†ç¦»æƒé‡è¡°å‡å‚æ•°
        no_decay = ["bias", "LayerNorm.weight"]
        optimizer_grouped_parameters = [
            {
                "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
                "weight_decay": TRAINING_CONFIG.weight_decay,
            },
            {
                "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
                "weight_decay": 0.0,
            },
        ]

        optimizer = AdamW(
            optimizer_grouped_parameters,
            lr=fine_tune_lr,
            eps=TRAINING_CONFIG.adam_epsilon,
            betas=(TRAINING_CONFIG.adam_beta1, TRAINING_CONFIG.adam_beta2),
        )

        logger.info(f"å¾®è°ƒå­¦ä¹ ç‡: {fine_tune_lr}")

        return optimizer

    def _create_scheduler(self) -> torch.optim.lr_scheduler._LRScheduler:
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        total_steps = len(self.train_dataloader) * TRAINING_CONFIG.num_epochs

        scheduler = LinearLR(self.optimizer, start_factor=1.0, end_factor=0.0, total_iters=total_steps)

        logger.info(f"å¾®è°ƒè°ƒåº¦å™¨åˆ›å»ºå®Œæˆï¼Œæ€»æ­¥æ•°: {total_steps}")

        return scheduler

    def train_epoch(self) -> Dict[str, float]:
        """
        è®­ç»ƒä¸€ä¸ªepoch

        Returns:
            epochæŒ‡æ ‡å­—å…¸
        """
        self.model.train()

        epoch_loss = 0.0
        num_batches = 0

        progress_bar = tqdm(
            self.train_dataloader,
            desc=f"å¾®è°ƒ Epoch {self.epoch + 1}/{TRAINING_CONFIG.num_epochs}",
            leave=False,
        )

        for step, batch in enumerate(progress_bar):
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            # batchåŒ…å«ï¼š
            # - input_ids: (batch_size, seq_len) token ids
            # - token_type_ids: (batch_size, seq_len) å¥å­ç±»å‹ids
            # - attention_mask: (batch_size, seq_len) æ³¨æ„åŠ›æ©ç 
            # - labels: (batch_size,) åˆ†ç±»æ ‡ç­¾
            batch = {k: v.to(self.device) for k, v in batch.items()}

            # å‰å‘ä¼ æ’­
            outputs = self.model(**batch)

            # è·å–æŸå¤±
            loss = outputs["loss"]

            # åå‘ä¼ æ’­
            loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), TRAINING_CONFIG.max_grad_norm)

            # ä¼˜åŒ–å™¨æ­¥éª¤
            self.optimizer.step()
            self.scheduler.step()
            self.optimizer.zero_grad()

            # æ›´æ–°ç»Ÿè®¡
            epoch_loss += loss.item()
            num_batches += 1
            self.global_step += 1

            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix(
                {
                    "loss": f"{loss.item():.4f}",
                    "lr": f"{self.scheduler.get_last_lr()[0]:.2e}",
                }
            )

        # è®¡ç®—epochå¹³å‡æŒ‡æ ‡
        avg_loss = epoch_loss / num_batches

        return {
            "train_loss": avg_loss,
            "learning_rate": self.scheduler.get_last_lr()[0],
        }

    def evaluate(self) -> Dict[str, float]:
        """
        è¯„ä¼°æ¨¡å‹

        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        self.model.eval()

        eval_loss = 0.0
        predictions = []
        true_labels = []
        num_batches = 0

        with torch.no_grad():
            for batch in tqdm(self.val_dataloader, desc="è¯„ä¼°", leave=False):
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                batch = {k: v.to(self.device) for k, v in batch.items()}

                # å‰å‘ä¼ æ’­
                outputs = self.model(**batch)

                # è·å–æŸå¤±å’Œé¢„æµ‹
                loss = outputs["loss"]
                logits = outputs["logits"]  # (batch_size, num_labels)

                # è®¡ç®—é¢„æµ‹æ ‡ç­¾
                preds = torch.argmax(logits, dim=-1)  # (batch_size,)

                # æ”¶é›†ç»“æœ
                eval_loss += loss.item()
                predictions.extend(preds.cpu().numpy())
                true_labels.extend(batch["labels"].cpu().numpy())
                num_batches += 1

        # è®¡ç®—æŒ‡æ ‡
        avg_loss = eval_loss / num_batches
        accuracy = accuracy_score(true_labels, predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average="weighted")

        return {
            "val_loss": avg_loss,
            "val_accuracy": accuracy,
            "val_precision": precision,
            "val_recall": recall,
            "val_f1": f1,
        }

    def fine_tune(self) -> Dict[str, Any]:
        """
        å®Œæ•´å¾®è°ƒæµç¨‹

        Returns:
            è®­ç»ƒå†å²
        """
        logger.info("å¼€å§‹BERTå¾®è°ƒ")
        logger.info(f"å¾®è°ƒè½®æ•°: {TRAINING_CONFIG.num_epochs}")
        logger.info(f"åˆ†ç±»æ ‡ç­¾æ•°: {self.num_labels}")

        training_start_time = time.time()

        for epoch in range(TRAINING_CONFIG.num_epochs):
            self.epoch = epoch
            epoch_start_time = time.time()

            # è®­ç»ƒä¸€ä¸ªepoch
            train_metrics = self.train_epoch()

            # è¯„ä¼°æ¨¡å‹
            eval_metrics = self.evaluate()

            # è®°å½•å†å²
            self.training_history["train_loss"].append(train_metrics["train_loss"])
            self.training_history["val_loss"].append(eval_metrics["val_loss"])
            self.training_history["val_accuracy"].append(eval_metrics["val_accuracy"])
            self.training_history["val_precision"].append(eval_metrics["val_precision"])
            self.training_history["val_recall"].append(eval_metrics["val_recall"])
            self.training_history["val_f1"].append(eval_metrics["val_f1"])
            self.training_history["learning_rate"].append(train_metrics["learning_rate"])
            self.training_history["epochs"].append(epoch + 1)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if eval_metrics["val_accuracy"] > self.best_accuracy:
                self.best_accuracy = eval_metrics["val_accuracy"]
                self._save_best_model()

            # è®°å½•epochæ€»ç»“
            epoch_time = time.time() - epoch_start_time
            logger.info(f"Epoch {epoch + 1} å®Œæˆ")
            logger.info(f"è®­ç»ƒæŸå¤±: {train_metrics['train_loss']:.4f}")
            logger.info(f"éªŒè¯æŸå¤±: {eval_metrics['val_loss']:.4f}")
            logger.info(f"éªŒè¯å‡†ç¡®ç‡: {eval_metrics['val_accuracy']:.4f}")
            logger.info(f"éªŒè¯F1: {eval_metrics['val_f1']:.4f}")
            logger.info(f"å­¦ä¹ ç‡: {train_metrics['learning_rate']:.2e}")
            logger.info(f"ç”¨æ—¶: {epoch_time:.2f}ç§’")

        # å¾®è°ƒå®Œæˆ
        total_time = time.time() - training_start_time
        logger.info(f"å¾®è°ƒå®Œæˆï¼Œæ€»ç”¨æ—¶: {total_time:.2f}ç§’")
        logger.info(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_accuracy:.4f}")

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self._save_final_model()

        # ä¿å­˜è®­ç»ƒå†å²
        self._save_training_history()

        return self.training_history

    def _save_best_model(self):
        """ä¿å­˜æœ€ä½³æ¨¡å‹"""
        best_model_dir = self.fine_tuning_save_dir / "best_model"
        best_model_dir.mkdir(exist_ok=True)

        torch.save(self.model.state_dict(), best_model_dir / "pytorch_model.bin")

        with open(best_model_dir / "config.json", "w") as f:
            config_dict = BERT_CONFIG.model_dump()
            config_dict["num_labels"] = self.num_labels
            json.dump(config_dict, f, indent=2)

        logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œå‡†ç¡®ç‡: {self.best_accuracy:.4f}")

    def _save_final_model(self):
        """ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
        final_model_dir = self.fine_tuning_save_dir / "final_model"
        final_model_dir.mkdir(exist_ok=True)

        torch.save(self.model.state_dict(), final_model_dir / "pytorch_model.bin")

        with open(final_model_dir / "config.json", "w") as f:
            config_dict = BERT_CONFIG.model_dump()
            config_dict["num_labels"] = self.num_labels
            json.dump(config_dict, f, indent=2)

        logger.info("ä¿å­˜æœ€ç»ˆæ¨¡å‹")

    def _save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        with open(self.fine_tuning_save_dir / "fine_tuning_history.json", "w") as f:
            json.dump(self.training_history, f, indent=2)

        logger.info("ä¿å­˜å¾®è°ƒå†å²")


def fine_tune_bert(pretrained_model_path: Optional[str] = None, num_labels: int = 2) -> Dict[str, Any]:
    """
    ä¾¿æ·çš„å¾®è°ƒå‡½æ•°

    Args:
        pretrained_model_path: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤è·¯å¾„
        num_labels: åˆ†ç±»æ ‡ç­¾æ•°é‡

    Returns:
        è®­ç»ƒå†å²
    """
    fine_tuner = BertFineTuner(pretrained_model_path, num_labels)
    history = fine_tuner.fine_tune()

    print("\nğŸ‰ å¾®è°ƒå®Œæˆï¼")
    print(f"æœ€ä½³å‡†ç¡®ç‡: {fine_tuner.best_accuracy:.4f}")
    print(f"å¾®è°ƒæ¨¡å‹ä¿å­˜ç›®å½•: {fine_tuner.fine_tuning_save_dir}")

    return history


def main():
    """ä¸»å¾®è°ƒå‡½æ•°"""
    import sys

    # æ”¯æŒå¯é€‰çš„é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„å‚æ•°
    if len(sys.argv) > 1:
        pretrained_model_path = sys.argv[1]
        print(f"ä½¿ç”¨æŒ‡å®šçš„é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„: {pretrained_model_path}")
    else:
        pretrained_model_path = None
        print(f"ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„: {TRAINING_CONFIG.pretrained_model_path}")

    history = fine_tune_bert(pretrained_model_path)
    print(f"è®­ç»ƒå†å²å·²ä¿å­˜")


if __name__ == "__main__":
    main()
