"""
é…ç½®æ¨¡å— - ç»Ÿä¸€çš„å‚æ•°ç®¡ç†
æ‰€æœ‰è¶…å‚æ•°éƒ½åœ¨è¿™é‡Œå®šä¹‰ï¼Œè¿è¡Œæ—¶ä¸éœ€è¦æ‰‹åŠ¨ä¼ å‚
"""

import torch
from pydantic import BaseModel, Field
from typing import Optional
import logging


class BertConfig(BaseModel):
    """BERTæ¨¡å‹é…ç½®"""

    # æ¨¡å‹æ¶æ„å‚æ•°
    vocab_size: int = Field(default=30522, description="è¯æ±‡è¡¨å¤§å°")
    hidden_size: int = Field(default=768, description="éšè—å±‚ç»´åº¦")
    num_hidden_layers: int = Field(default=12, description="Transformerå±‚æ•°")
    num_attention_heads: int = Field(default=12, description="æ³¨æ„åŠ›å¤´æ•°")
    intermediate_size: int = Field(default=3072, description="å‰é¦ˆç½‘ç»œä¸­é—´å±‚ç»´åº¦")
    hidden_act: str = Field(default="gelu", description="æ¿€æ´»å‡½æ•°")

    # ä½ç½®å’Œç±»å‹åµŒå…¥
    max_position_embeddings: int = Field(default=512, description="æœ€å¤§ä½ç½®åµŒå…¥æ•°")
    type_vocab_size: int = Field(default=2, description="tokenç±»å‹è¯æ±‡è¡¨å¤§å°")

    # æ­£åˆ™åŒ–å‚æ•°
    hidden_dropout_prob: float = Field(default=0.1, description="éšè—å±‚dropoutæ¦‚ç‡")
    attention_probs_dropout_prob: float = Field(default=0.1, description="æ³¨æ„åŠ›dropoutæ¦‚ç‡")
    layer_norm_eps: float = Field(default=1e-12, description="LayerNormçš„epsilon")

    # åˆå§‹åŒ–å‚æ•°
    initializer_range: float = Field(default=0.02, description="æƒé‡åˆå§‹åŒ–èŒƒå›´")

    # ä»»åŠ¡ç±»å‹ï¼ˆç”¨äºåˆ†ç±»ä»»åŠ¡ï¼‰
    problem_type: Optional[str] = Field(default=None, description="é—®é¢˜ç±»å‹")

    class Config:
        """Pydanticé…ç½®"""

        extra = "forbid"  # ç¦æ­¢é¢å¤–å­—æ®µ


class TrainingConfig(BaseModel):
    """è®­ç»ƒé…ç½®"""

    # åŸºç¡€è®­ç»ƒå‚æ•°
    batch_size: int = Field(default=16, description="æ‰¹æ¬¡å¤§å°")
    learning_rate: float = Field(default=1e-4, description="å­¦ä¹ ç‡")
    num_epochs: int = Field(default=3, description="è®­ç»ƒè½®æ•°")
    warmup_steps: int = Field(default=1000, description="é¢„çƒ­æ­¥æ•°")

    # ä¼˜åŒ–å™¨å‚æ•°
    weight_decay: float = Field(default=0.01, description="æƒé‡è¡°å‡")
    adam_epsilon: float = Field(default=1e-8, description="Adamä¼˜åŒ–å™¨epsilon")
    adam_beta1: float = Field(default=0.9, description="Adamä¼˜åŒ–å™¨beta1")
    adam_beta2: float = Field(default=0.999, description="Adamä¼˜åŒ–å™¨beta2")
    max_grad_norm: float = Field(default=1.0, description="æ¢¯åº¦è£å‰ªé˜ˆå€¼")

    # æ•°æ®é›†å‚æ•°
    dataset_name: str = Field(default="Salesforce/wikitext", description="æ•°æ®é›†åç§°")
    dataset_config: str = Field(default="wikitext-2-raw-v1", description="æ•°æ®é›†é…ç½®")
    max_samples: Optional[int] = Field(default=1000, description="æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰")

    # è®¾å¤‡å’Œå¹¶è¡Œ
    device: str = Field(default="auto", description="è®­ç»ƒè®¾å¤‡")
    num_workers: int = Field(default=4, description="æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°")

    # æ—¥å¿—å’Œä¿å­˜
    logging_steps: int = Field(default=100, description="æ—¥å¿—è®°å½•æ­¥æ•°")
    save_steps: int = Field(default=1000, description="æ¨¡å‹ä¿å­˜æ­¥æ•°")

    # ç›®å½•é…ç½® - æ‰€æœ‰è·¯å¾„éƒ½åœ¨è¿™é‡Œç»Ÿä¸€å®šä¹‰
    # é¢„è®­ç»ƒç›¸å…³ç›®å½•
    pretrain_checkpoints_dir: str = Field(
        default="/Users/liuqianli/work/python/deepai/saved_model/bert/pretrain/checkpoints", description="é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•"
    )
    pretrain_best_dir: str = Field(
        default="/Users/liuqianli/work/python/deepai/saved_model/bert/pretrain/best", description="é¢„è®­ç»ƒæœ€ä½³æ¨¡å‹ä¿å­˜ç›®å½•"
    )
    pretrain_final_dir: str = Field(
        default="/Users/liuqianli/work/python/deepai/saved_model/bert/pretrain/final", description="é¢„è®­ç»ƒæœ€ç»ˆæ¨¡å‹ä¿å­˜ç›®å½•"
    )

    # å¾®è°ƒç›¸å…³ç›®å½•
    finetuning_checkpoints_dir: str = Field(
        default="/Users/liuqianli/work/python/deepai/saved_model/bert/finetuning/checkpoints", description="å¾®è°ƒè¿‡ç¨‹ä¸­çš„æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•"
    )
    finetuning_best_dir: str = Field(
        default="/Users/liuqianli/work/python/deepai/saved_model/bert/finetuning/best", description="å¾®è°ƒæœ€ä½³æ¨¡å‹ä¿å­˜ç›®å½•"
    )
    finetuning_final_dir: str = Field(
        default="/Users/liuqianli/work/python/deepai/saved_model/bert/finetuning/final", description="å¾®è°ƒæœ€ç»ˆæ¨¡å‹ä¿å­˜ç›®å½•"
    )

    # å…¶ä»–ç›®å½•
    log_dir: str = Field(default="/Users/liuqianli/work/python/deepai/logs/bert", description="æ—¥å¿—ä¿å­˜ç›®å½•")
    cache_dir: str = Field(default="/Users/liuqianli/.cache/huggingface/datasets", description="HuggingFaceæ•°æ®é›†ç¼“å­˜ç›®å½•")

    class Config:
        """Pydanticé…ç½®"""

        extra = "forbid"


class DataConfig(BaseModel):
    """æ•°æ®é…ç½®"""

    # Tokenizeré…ç½®
    tokenizer_name: str = Field(default="bert-base-uncased", description="tokenizeråç§°")
    do_lower_case: bool = Field(default=True, description="æ˜¯å¦è½¬æ¢ä¸ºå°å†™")

    # åºåˆ—é•¿åº¦
    max_length: int = Field(default=128, description="æœ€å¤§åºåˆ—é•¿åº¦")

    # MLMé…ç½®
    mlm_probability: float = Field(default=0.15, description="MLMæ©ç æ¦‚ç‡")

    # NSPé…ç½®
    nsp_probability: float = Field(default=0.5, description="NSPè´Ÿæ ·æœ¬æ¦‚ç‡")

    class Config:
        """Pydanticé…ç½®"""

        extra = "forbid"


class LoggingConfig(BaseModel):
    """æ—¥å¿—é…ç½®"""

    log_level: str = Field(default="INFO", description="æ—¥å¿—çº§åˆ«")
    log_format: str = Field(
        default="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        description="æ—¥å¿—æ ¼å¼",
    )
    log_file: Optional[str] = Field(default=None, description="æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼Œå°†ä½¿ç”¨TRAINING_CONFIG.log_dir")

    class Config:
        """Pydanticé…ç½®"""

        extra = "forbid"


# å…¨å±€é…ç½®å®ä¾‹ - è¿è¡Œæ—¶ç›´æ¥ä½¿ç”¨è¿™äº›é…ç½®ï¼Œä¸éœ€è¦æ‰‹åŠ¨ä¼ å‚
BERT_CONFIG = BertConfig()
TRAINING_CONFIG = TrainingConfig()
DATA_CONFIG = DataConfig()
LOGGING_CONFIG = LoggingConfig()


def get_device() -> torch.device:
    """
    è‡ªåŠ¨æ£€æµ‹å¹¶è¿”å›æœ€ä½³è®¾å¤‡

    Returns:
        torch.device: è®¾å¤‡å¯¹è±¡
    """
    if TRAINING_CONFIG.device == "auto":
        if torch.cuda.is_available():
            device = torch.device("cuda")
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            device = torch.device("mps")
        else:
            device = torch.device("cpu")
    else:
        device = torch.device(TRAINING_CONFIG.device)

    return device


def setup_logging():
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    import os
    from datetime import datetime

    # åˆ›å»ºæ—¥å¿—ç›®å½•
    log_dir = TRAINING_CONFIG.log_dir
    os.makedirs(log_dir, exist_ok=True)

    # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶å
    log_filename = f"bert_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    log_filepath = os.path.join(log_dir, log_filename)

    logging.basicConfig(
        level=getattr(logging, LOGGING_CONFIG.log_level),
        format=LOGGING_CONFIG.log_format,
        handlers=[logging.StreamHandler(), logging.FileHandler(log_filepath, encoding="utf-8")],  # æ§åˆ¶å°è¾“å‡º  # æ–‡ä»¶è¾“å‡º
    )

    # åˆ›å»ºBERTä¸“ç”¨logger
    logger = logging.getLogger("BERT")
    logger.setLevel(getattr(logging, LOGGING_CONFIG.log_level))

    return logger


def create_directories():
    """åˆ›å»ºæ‰€æœ‰å¿…è¦çš„ç›®å½•"""
    import os

    directories = [
        # é¢„è®­ç»ƒç›¸å…³ç›®å½•
        TRAINING_CONFIG.pretrain_checkpoints_dir,
        TRAINING_CONFIG.pretrain_best_dir,
        TRAINING_CONFIG.pretrain_final_dir,
        # å¾®è°ƒç›¸å…³ç›®å½•
        TRAINING_CONFIG.finetuning_checkpoints_dir,
        TRAINING_CONFIG.finetuning_best_dir,
        TRAINING_CONFIG.finetuning_final_dir,
        # å…¶ä»–ç›®å½•
        TRAINING_CONFIG.log_dir,
        TRAINING_CONFIG.cache_dir,
    ]

    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        print(f"âœ… åˆ›å»ºç›®å½•: {directory}")


def print_config():
    """æ‰“å°æ‰€æœ‰é…ç½®ä¿¡æ¯"""
    print("=" * 50)
    print("BERTæ¡†æ¶é…ç½®ä¿¡æ¯")
    print("=" * 50)

    print("\nğŸ—ï¸ æ¨¡å‹é…ç½®:")
    print(f"  è¯æ±‡è¡¨å¤§å°: {BERT_CONFIG.vocab_size:,}")
    print(f"  éšè—å±‚ç»´åº¦: {BERT_CONFIG.hidden_size}")
    print(f"  Transformerå±‚æ•°: {BERT_CONFIG.num_hidden_layers}")
    print(f"  æ³¨æ„åŠ›å¤´æ•°: {BERT_CONFIG.num_attention_heads}")
    print(f"  æœ€å¤§åºåˆ—é•¿åº¦: {DATA_CONFIG.max_length}")

    print("\nğŸš€ è®­ç»ƒé…ç½®:")
    print(f"  æ‰¹æ¬¡å¤§å°: {TRAINING_CONFIG.batch_size}")
    print(f"  å­¦ä¹ ç‡: {TRAINING_CONFIG.learning_rate}")
    print(f"  è®­ç»ƒè½®æ•°: {TRAINING_CONFIG.num_epochs}")
    print(f"  æœ€å¤§æ ·æœ¬æ•°: {TRAINING_CONFIG.max_samples}")
    print(f"  è®¾å¤‡: {get_device()}")

    print("\nğŸ“Š æ•°æ®é…ç½®:")
    print(f"  æ•°æ®é›†: {TRAINING_CONFIG.dataset_name}")
    print(f"  é…ç½®: {TRAINING_CONFIG.dataset_config}")
    print(f"  Tokenizer: {DATA_CONFIG.tokenizer_name}")
    print(f"  MLMæ¦‚ç‡: {DATA_CONFIG.mlm_probability}")

    print("\nğŸ“ ç›®å½•é…ç½®:")
    print("  é¢„è®­ç»ƒç›¸å…³ç›®å½•:")
    print(f"    æ£€æŸ¥ç‚¹ç›®å½•: {TRAINING_CONFIG.pretrain_checkpoints_dir}")
    print(f"    æœ€ä½³æ¨¡å‹ç›®å½•: {TRAINING_CONFIG.pretrain_best_dir}")
    print(f"    æœ€ç»ˆæ¨¡å‹ç›®å½•: {TRAINING_CONFIG.pretrain_final_dir}")
    print("  å¾®è°ƒç›¸å…³ç›®å½•:")
    print(f"    æ£€æŸ¥ç‚¹ç›®å½•: {TRAINING_CONFIG.finetuning_checkpoints_dir}")
    print(f"    æœ€ä½³æ¨¡å‹ç›®å½•: {TRAINING_CONFIG.finetuning_best_dir}")
    print(f"    æœ€ç»ˆæ¨¡å‹ç›®å½•: {TRAINING_CONFIG.finetuning_final_dir}")
    print("  å…¶ä»–ç›®å½•:")
    print(f"    æ—¥å¿—ä¿å­˜ç›®å½•: {TRAINING_CONFIG.log_dir}")
    print(f"    æ•°æ®ç¼“å­˜ç›®å½•: {TRAINING_CONFIG.cache_dir}")
    print(f"    æ—¥å¿—çº§åˆ«: {LOGGING_CONFIG.log_level}")

    print("=" * 50)


if __name__ == "__main__":
    # æµ‹è¯•é…ç½®
    print_config()

    # æµ‹è¯•è®¾å¤‡æ£€æµ‹
    device = get_device()
    print(f"\næ£€æµ‹åˆ°çš„è®¾å¤‡: {device}")

    # æµ‹è¯•æ—¥å¿—
    logger = setup_logging()
    logger.info("é…ç½®æ¨¡å—æµ‹è¯•æˆåŠŸï¼")
