"""
è®­ç»ƒå™¨æ¨¡å— - BERTé¢„è®­ç»ƒè®­ç»ƒå™¨
åŒ…å«å®Œæ•´çš„è®­ç»ƒå¾ªç¯ã€ä¼˜åŒ–å™¨ã€å­¦ä¹ ç‡è°ƒåº¦å™¨ç­‰
é‡ç‚¹å…³æ³¨è®­ç»ƒè¿‡ç¨‹çš„æ•°æ®æµè½¬ï¼ŒåŒ…å«è¯¦ç»†çš„shapeæ³¨é‡Š
"""

import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.optim.lr_scheduler import LinearLR
import os
import json
import logging
from typing import Dict, Any, Optional, Tuple
from tqdm import tqdm
import time
from pathlib import Path
import numpy as np

from config import BERT_CONFIG, TRAINING_CONFIG, get_device, setup_logging
from model import BertForPreTraining
from data_loader import create_pretraining_dataloader

logger = logging.getLogger("BERT")


class BertTrainer:
    """
    BERTé¢„è®­ç»ƒè®­ç»ƒå™¨

    è´Ÿè´£å®Œæ•´çš„é¢„è®­ç»ƒæµç¨‹ï¼š
    1. æ¨¡å‹åˆå§‹åŒ–
    2. æ•°æ®åŠ è½½
    3. ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨è®¾ç½®
    4. è®­ç»ƒå¾ªç¯
    5. æ¨¡å‹ä¿å­˜å’Œè¯„ä¼°
    """

    def __init__(self):
        """åˆå§‹åŒ–è®­ç»ƒå™¨"""
        # è®¾ç½®æ—¥å¿—
        setup_logging()

        # è®¾å¤‡é…ç½®
        self.device = get_device()
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

        # åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•
        self.model_save_dir = Path(TRAINING_CONFIG.model_save_dir)
        self.model_save_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–æ¨¡å‹
        self.model = self._create_model()

        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        self.train_dataloader, self.tokenizer = self._create_dataloader()

        # åˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self.optimizer = self._create_optimizer()
        self.scheduler = self._create_scheduler()

        # è®­ç»ƒçŠ¶æ€
        self.global_step = 0
        self.epoch = 0
        self.best_loss = float("inf")

        # è®­ç»ƒå†å²
        self.training_history = {
            "train_loss": [],
            "mlm_loss": [],
            "nsp_loss": [],
            "learning_rate": [],
            "steps": [],
        }

        logger.info("è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")

    def _create_model(self) -> BertForPreTraining:
        """
        åˆ›å»ºBERTé¢„è®­ç»ƒæ¨¡å‹

        Returns:
            BertForPreTraining: é¢„è®­ç»ƒæ¨¡å‹
        """
        logger.info("åˆ›å»ºBERTé¢„è®­ç»ƒæ¨¡å‹...")

        # æ›´æ–°è¯æ±‡è¡¨å¤§å°ï¼ˆæ ¹æ®å®é™…tokenizerï¼‰
        model = BertForPreTraining()
        model.to(self.device)

        logger.info(f"æ¨¡å‹å‚æ•°æ•°é‡: {model.count_parameters():,}")
        logger.info(f"æ¨¡å‹å¤§å°: {model.count_parameters() * 4 / 1024 / 1024:.2f} MB")

        return model

    def _create_dataloader(self) -> Tuple[torch.utils.data.DataLoader, Any]:
        """
        åˆ›å»ºæ•°æ®åŠ è½½å™¨

        Returns:
            (æ•°æ®åŠ è½½å™¨, tokenizer)
        """
        logger.info("åˆ›å»ºé¢„è®­ç»ƒæ•°æ®åŠ è½½å™¨...")

        dataloader, tokenizer = create_pretraining_dataloader()

        # æ›´æ–°æ¨¡å‹çš„è¯æ±‡è¡¨å¤§å°
        actual_vocab_size = len(tokenizer)
        if actual_vocab_size != BERT_CONFIG.vocab_size:
            logger.info(f"æ›´æ–°è¯æ±‡è¡¨å¤§å°: {BERT_CONFIG.vocab_size} -> {actual_vocab_size}")
            BERT_CONFIG.vocab_size = actual_vocab_size

            # é‡æ–°åˆ›å»ºæ¨¡å‹ä»¥åŒ¹é…è¯æ±‡è¡¨å¤§å°
            self.model = BertForPreTraining()
            self.model.to(self.device)

        logger.info(f"æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼Œæ‰¹æ¬¡æ•°é‡: {len(dataloader)}")

        return dataloader, tokenizer

    def _create_optimizer(self) -> torch.optim.Optimizer:
        """
        åˆ›å»ºä¼˜åŒ–å™¨

        Returns:
            AdamWä¼˜åŒ–å™¨
        """
        logger.info("åˆ›å»ºä¼˜åŒ–å™¨...")

        # åˆ†ç¦»æƒé‡è¡°å‡å‚æ•°
        no_decay = ["bias", "LayerNorm.weight"]
        optimizer_grouped_parameters = [
            {
                "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
                "weight_decay": TRAINING_CONFIG.weight_decay,
            },
            {
                "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
                "weight_decay": 0.0,
            },
        ]

        optimizer = AdamW(
            optimizer_grouped_parameters,
            lr=TRAINING_CONFIG.learning_rate,
            eps=TRAINING_CONFIG.adam_epsilon,
            betas=(TRAINING_CONFIG.adam_beta1, TRAINING_CONFIG.adam_beta2),
        )

        logger.info(f"AdamWä¼˜åŒ–å™¨åˆ›å»ºå®Œæˆï¼Œå­¦ä¹ ç‡: {TRAINING_CONFIG.learning_rate}")

        return optimizer

    def _create_scheduler(self) -> torch.optim.lr_scheduler._LRScheduler:
        """
        åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨

        Returns:
            å­¦ä¹ ç‡è°ƒåº¦å™¨
        """
        total_steps = len(self.train_dataloader) * TRAINING_CONFIG.num_epochs

        # çº¿æ€§è¡°å‡è°ƒåº¦å™¨
        scheduler = LinearLR(self.optimizer, start_factor=1.0, end_factor=0.0, total_iters=total_steps)

        logger.info(f"å­¦ä¹ ç‡è°ƒåº¦å™¨åˆ›å»ºå®Œæˆï¼Œæ€»æ­¥æ•°: {total_steps}")

        return scheduler

    def train_epoch(self) -> Dict[str, float]:
        """
        è®­ç»ƒä¸€ä¸ªepoch

        Returns:
            epochæŒ‡æ ‡å­—å…¸
        """
        self.model.train()

        epoch_loss = 0.0
        epoch_mlm_loss = 0.0
        epoch_nsp_loss = 0.0
        num_batches = 0

        progress_bar = tqdm(
            self.train_dataloader,
            desc=f"é¢„è®­ç»ƒ Epoch {self.epoch + 1}/{TRAINING_CONFIG.num_epochs}",
            leave=False,
        )

        for step, batch in enumerate(progress_bar):
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            # batchåŒ…å«ï¼š
            # - input_ids: (batch_size, seq_len) æ©ç åçš„token ids
            # - token_type_ids: (batch_size, seq_len) å¥å­ç±»å‹ids
            # - attention_mask: (batch_size, seq_len) æ³¨æ„åŠ›æ©ç 
            # - labels: (batch_size, seq_len) MLMæ ‡ç­¾
            # - next_sentence_label: (batch_size,) NSPæ ‡ç­¾
            batch = {k: v.to(self.device) for k, v in batch.items()}

            # å‰å‘ä¼ æ’­
            outputs = self.model(**batch)

            # è·å–æŸå¤±
            total_loss = outputs["loss"]  # MLMæŸå¤± + NSPæŸå¤±

            # è®¡ç®—å•ç‹¬çš„æŸå¤±ï¼ˆç”¨äºç›‘æ§ï¼‰
            # ä¸Šé¢self.model(**batch)é‡Œå¯èƒ½ä¹Ÿä¼šè®¡ç®—mlm_losså’Œnsp_loss
            # ä¸Šé¢ modelå’Œè¿™é‡Œçš„è®¡ç®—æ–¹å¼æ˜¯ä¸€è‡´çš„ï¼Œä¸¤å¤„éƒ½è®¡ç®—äº†ï¼Œåªæ˜¯å·¥ç¨‹é—®é¢˜ï¼Œä¸å½±å“ç†è§£ä»£ç 
            mlm_loss = self._compute_mlm_loss(outputs, batch)
            nsp_loss = self._compute_nsp_loss(outputs, batch)

            # åå‘ä¼ æ’­
            total_loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), TRAINING_CONFIG.max_grad_norm)

            # ä¼˜åŒ–å™¨æ­¥éª¤
            # #self.optimizer.step()
            # ä¼˜åŒ–å™¨ï¼ˆå¦‚ Adamã€SGD ç­‰ï¼‰æ ¹æ®åå‘ä¼ æ’­è®¡ç®—å‡ºçš„æ¢¯åº¦ï¼ˆé€šè¿‡ loss.backward() å¾—åˆ°ï¼‰ï¼Œå¯¹æ¨¡å‹å‚æ•°è¿›è¡Œ å®é™…æ›´æ–° ã€‚
            # è¿™ä¸€æ­¥æ˜¯æ¨¡å‹å­¦ä¹ çš„å…³é”®â€”â€”é€šè¿‡è°ƒæ•´å‚æ•°ä½¿æŸå¤±å‡½æ•°é€æ­¥é™ä½ã€‚
            # 2. self.scheduler.step()
            # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¦‚ä½™å¼¦é€€ç«è°ƒåº¦å™¨ã€çº¿æ€§è¡°å‡è°ƒåº¦å™¨ç­‰ï¼‰æ ¹æ®å½“å‰è®­ç»ƒçŠ¶æ€ï¼ˆå¦‚æ­¥æ•°ã€epoch æ•°æˆ–æŸå¤±å€¼ï¼‰è°ƒæ•´ä¼˜åŒ–å™¨çš„ å­¦ä¹ ç‡ã€‚
            # å­¦ä¹ ç‡æ˜¯è®­ç»ƒçš„æ ¸å¿ƒè¶…å‚æ•°ï¼Œåˆç†è°ƒæ•´å¯åŠ é€Ÿæ”¶æ•›æˆ–é¿å…è¿‡æ‹Ÿåˆã€‚
            # æ‰§è¡Œé¡ºåºè¯´æ˜
            # é€šå¸¸å…ˆæ‰§è¡Œ optimizer.step() æ›´æ–°å‚æ•°ï¼Œå†æ‰§è¡Œ scheduler.step() æ›´æ–°å­¦ä¹ ç‡ã€‚è¿™æ˜¯å› ä¸ºå­¦ä¹ ç‡è°ƒåº¦å™¨å¯èƒ½ä¾èµ–å½“å‰æ­¥æ•°ï¼ˆå¦‚çƒ­èº«ç­–ç•¥åœ¨åˆå§‹é˜¶æ®µé€æ¸å¢åŠ å­¦ä¹ ç‡ï¼‰ï¼Œè€Œå‚æ•°æ›´æ–°åæ‰éœ€è¦è°ƒæ•´ä¸‹ä¸€æ¬¡æ›´æ–°çš„å­¦ä¹ ç‡ã€‚
            self.optimizer.step()
            self.scheduler.step()
            self.optimizer.zero_grad()

            # æ›´æ–°ç»Ÿè®¡
            epoch_loss += total_loss.item()
            epoch_mlm_loss += mlm_loss.item()
            epoch_nsp_loss += nsp_loss.item()
            num_batches += 1

            self.global_step += 1

            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix(
                {
                    "loss": f"{total_loss.item():.4f}",
                    "mlm": f"{mlm_loss.item():.4f}",
                    "nsp": f"{nsp_loss.item():.4f}",
                    "lr": f"{self.scheduler.get_last_lr()[0]:.2e}",
                }
            )

            # è®°å½•è®­ç»ƒæ—¥å¿—
            if self.global_step % TRAINING_CONFIG.logging_steps == 0:
                self._log_training_step(total_loss.item(), mlm_loss.item(), nsp_loss.item())

            # ä¿å­˜æ£€æŸ¥ç‚¹
            if self.global_step % TRAINING_CONFIG.save_steps == 0:
                self._save_checkpoint()

        # è®¡ç®—epochå¹³å‡æŒ‡æ ‡
        avg_loss = epoch_loss / num_batches
        avg_mlm_loss = epoch_mlm_loss / num_batches
        avg_nsp_loss = epoch_nsp_loss / num_batches

        return {
            "train_loss": avg_loss,
            "mlm_loss": avg_mlm_loss,
            "nsp_loss": avg_nsp_loss,
            "learning_rate": self.scheduler.get_last_lr()[0],
        }

    def _compute_mlm_loss(self, outputs: Dict[str, torch.Tensor], batch: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        è®¡ç®—MLMæŸå¤±

        Args:
            outputs: æ¨¡å‹è¾“å‡º
            batch: æ‰¹æ¬¡æ•°æ®

        Returns:
            MLMæŸå¤±
        """
        prediction_logits = outputs["prediction_logits"]  # (batch_size, seq_len, vocab_size)
        labels = batch["labels"]  # (batch_size, seq_len)

        loss_fct = nn.CrossEntropyLoss()
        mlm_loss = loss_fct(
            prediction_logits.view(-1, BERT_CONFIG.vocab_size),  # (batch_size * seq_len, vocab_size)
            labels.view(-1),  # (batch_size * seq_len,)
        )

        return mlm_loss

    def _compute_nsp_loss(self, outputs: Dict[str, torch.Tensor], batch: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        è®¡ç®—NSPæŸå¤±

        Args:
            outputs: æ¨¡å‹è¾“å‡º
            batch: æ‰¹æ¬¡æ•°æ®

        Returns:
            NSPæŸå¤±
        """
        seq_relationship_logits = outputs["seq_relationship_logits"]  # (batch_size, 2)
        next_sentence_labels = batch["next_sentence_label"]  # (batch_size,)

        loss_fct = nn.CrossEntropyLoss()
        nsp_loss = loss_fct(
            seq_relationship_logits.view(-1, 2),  # (batch_size, 2)
            next_sentence_labels.view(-1),  # (batch_size,)
        )

        return nsp_loss

    def _log_training_step(self, total_loss: float, mlm_loss: float, nsp_loss: float):
        """è®°å½•è®­ç»ƒæ­¥éª¤"""
        logger.info(
            f"Step {self.global_step}: "
            f"total_loss={total_loss:.4f}, "
            f"mlm_loss={mlm_loss:.4f}, "
            f"nsp_loss={nsp_loss:.4f}, "
            f"lr={self.scheduler.get_last_lr()[0]:.2e}"
        )

    def _save_checkpoint(self):
        """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹"""
        checkpoint_dir = self.model_save_dir / f"checkpoint-{self.global_step}"
        checkpoint_dir.mkdir(exist_ok=True)

        # ä¿å­˜æ¨¡å‹çŠ¶æ€
        torch.save(self.model.state_dict(), checkpoint_dir / "pytorch_model.bin")

        # ä¿å­˜é…ç½®
        with open(checkpoint_dir / "config.json", "w") as f:
            json.dump(BERT_CONFIG.model_dump(), f, indent=2)

        # ä¿å­˜è®­ç»ƒçŠ¶æ€
        training_state = {
            "global_step": self.global_step,
            "epoch": self.epoch,
            "best_loss": self.best_loss,
            "optimizer_state": self.optimizer.state_dict(),
            "scheduler_state": self.scheduler.state_dict(),
        }

        torch.save(training_state, checkpoint_dir / "training_state.bin")

        logger.info(f"ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_dir}")

    def train(self) -> Dict[str, Any]:
        """
        å®Œæ•´è®­ç»ƒæµç¨‹

        Returns:
            è®­ç»ƒå†å²
        """
        logger.info("å¼€å§‹BERTé¢„è®­ç»ƒ")
        logger.info(f"è®­ç»ƒè½®æ•°: {TRAINING_CONFIG.num_epochs}")
        logger.info(f"æ‰¹æ¬¡å¤§å°: {TRAINING_CONFIG.batch_size}")
        logger.info(f"å­¦ä¹ ç‡: {TRAINING_CONFIG.learning_rate}")

        training_start_time = time.time()

        for epoch in range(TRAINING_CONFIG.num_epochs):
            self.epoch = epoch
            epoch_start_time = time.time()

            # è®­ç»ƒä¸€ä¸ªepoch
            epoch_metrics = self.train_epoch()

            # è®°å½•å†å²
            self.training_history["train_loss"].append(epoch_metrics["train_loss"])
            self.training_history["mlm_loss"].append(epoch_metrics["mlm_loss"])
            self.training_history["nsp_loss"].append(epoch_metrics["nsp_loss"])
            self.training_history["learning_rate"].append(epoch_metrics["learning_rate"])
            self.training_history["steps"].append(self.global_step)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if epoch_metrics["train_loss"] < self.best_loss:
                self.best_loss = epoch_metrics["train_loss"]
                self._save_best_model()

            # è®°å½•epochæ€»ç»“
            epoch_time = time.time() - epoch_start_time
            logger.info(f"Epoch {epoch + 1} å®Œæˆ")
            logger.info(f"è®­ç»ƒæŸå¤±: {epoch_metrics['train_loss']:.4f}")
            logger.info(f"MLMæŸå¤±: {epoch_metrics['mlm_loss']:.4f}")
            logger.info(f"NSPæŸå¤±: {epoch_metrics['nsp_loss']:.4f}")
            logger.info(f"å­¦ä¹ ç‡: {epoch_metrics['learning_rate']:.2e}")
            logger.info(f"ç”¨æ—¶: {epoch_time:.2f}ç§’")

        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - training_start_time
        logger.info(f"é¢„è®­ç»ƒå®Œæˆï¼Œæ€»ç”¨æ—¶: {total_time:.2f}ç§’")
        logger.info(f"æœ€ä½³è®­ç»ƒæŸå¤±: {self.best_loss:.4f}")

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self._save_final_model()

        # ä¿å­˜è®­ç»ƒå†å²
        self._save_training_history()

        return self.training_history

    def _save_best_model(self):
        """ä¿å­˜æœ€ä½³æ¨¡å‹"""
        best_model_dir = self.model_save_dir / "best_model"
        best_model_dir.mkdir(exist_ok=True)

        torch.save(self.model.state_dict(), best_model_dir / "pytorch_model.bin")

        with open(best_model_dir / "config.json", "w") as f:
            json.dump(BERT_CONFIG.model_dump(), f, indent=2)

        logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒæŸå¤±: {self.best_loss:.4f}")

    def _save_final_model(self):
        """ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
        final_model_dir = self.model_save_dir / "final_model"
        final_model_dir.mkdir(exist_ok=True)

        torch.save(self.model.state_dict(), final_model_dir / "pytorch_model.bin")

        with open(final_model_dir / "config.json", "w") as f:
            json.dump(BERT_CONFIG.model_dump(), f, indent=2)

        logger.info("ä¿å­˜æœ€ç»ˆæ¨¡å‹")

    def _save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        with open(self.model_save_dir / "training_history.json", "w") as f:
            json.dump(self.training_history, f, indent=2)

        logger.info("ä¿å­˜è®­ç»ƒå†å²")


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    trainer = BertTrainer()
    history = trainer.train()

    print("\nğŸ‰ é¢„è®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³æŸå¤±: {trainer.best_loss:.4f}")
    print(f"æ¨¡å‹ä¿å­˜ç›®å½•: {trainer.model_save_dir}")


if __name__ == "__main__":
    main()
