# BERT2 - ä»é›¶å®ç°çš„BERTæ¡†æ¶

è¿™æ˜¯ä¸€ä¸ªä»é›¶å®ç°çš„BERTæ¡†æ¶ï¼Œä¸“ä¸ºå­¦ä¹ ç›®çš„è®¾è®¡ã€‚ä»£ç ç»“æ„æ¸…æ™°ï¼Œæ³¨é‡Šè¯¦ç»†ï¼ŒåŒ…å«å®Œæ•´çš„é¢„è®­ç»ƒã€å¾®è°ƒå’Œæ¨ç†åŠŸèƒ½ã€‚

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ¨¡å—ç»„ç»‡
```
bert2/
â”œâ”€â”€ config.py          # é…ç½®æ¨¡å— - ç»Ÿä¸€å‚æ•°ç®¡ç†
â”œâ”€â”€ transformer.py     # TransformeråŸºç¡€ç»„ä»¶
â”œâ”€â”€ model.py           # BERTæ¨¡å‹å®ç°
â”œâ”€â”€ data_loader.py     # æ•°æ®åŠ è½½å™¨
â”œâ”€â”€ trainer.py         # é¢„è®­ç»ƒè®­ç»ƒå™¨
â”œâ”€â”€ fine_tuning.py     # å¾®è°ƒæ¨¡å—
â”œâ”€â”€ inference.py       # æ¨ç†æ¨¡å—
â”œâ”€â”€ main.py           # ä¸»è¿è¡Œè„šæœ¬
â”œâ”€â”€ requirements.txt   # ä¾èµ–åŒ…
â””â”€â”€ README.md         # è¯´æ˜æ–‡æ¡£
```

### æ ¸å¿ƒç‰¹æ€§
- âœ… **å®Œæ•´çš„BERTå®ç°**ï¼šåŒ…å«MLMå’ŒNSPé¢„è®­ç»ƒä»»åŠ¡
- âœ… **è¯¦ç»†çš„shapeæ³¨é‡Š**ï¼šæ¯ä¸ªtensoréƒ½æœ‰è¯¦ç»†çš„å½¢çŠ¶è¯´æ˜
- âœ… **æ¸…æ™°çš„æ•°æ®æµè½¬**ï¼šé‡ç‚¹è§£é‡Šmaskåˆ›å»ºå’Œä½¿ç”¨é€»è¾‘
- âœ… **æ¨¡å—åŒ–è®¾è®¡**ï¼šå„æ¨¡å—èŒè´£æ¸…æ™°ï¼Œæ˜“äºç†è§£å’Œæ‰©å±•
- âœ… **é…ç½®é©±åŠ¨**ï¼šæ‰€æœ‰å‚æ•°åœ¨config.pyä¸­ç»Ÿä¸€ç®¡ç†
- âœ… **å®Œå–„çš„æ—¥å¿—**ï¼šè¯¦ç»†çš„è®­ç»ƒå’Œæ¨ç†æ—¥å¿—
- âœ… **æ”¯æŒå¾®è°ƒ**ï¼šä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æƒé‡è¿›è¡Œåˆ†ç±»ä»»åŠ¡å¾®è°ƒ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–
```bash
pip install -r requirements.txt
```

### 2. å¿«é€Ÿæµ‹è¯•
```bash
# è¿è¡Œå¿«é€Ÿæµ‹è¯•ï¼ˆå°è§„æ¨¡é…ç½®ï¼Œé€‚åˆå­¦ä¹ ï¼‰
python main.py quick
```

### 3. å®Œæ•´è®­ç»ƒ
```bash
# é¢„è®­ç»ƒ
python main.py pretrain

# å¾®è°ƒ
python main.py finetune

# æˆ–è€…è¿è¡Œå®Œæ•´æµç¨‹
python main.py full
```

### 4. æ¨ç†æµ‹è¯•
```bash
# é¢„è®­ç»ƒæ¨¡å‹æ¨ç†ï¼ˆæ©ç é¢„æµ‹ï¼‰
python main.py inference --model_type pretraining

# åˆ†ç±»æ¨¡å‹æ¨ç†
python main.py inference --model_type classification
```

## âš™ï¸ é…ç½®è¯´æ˜

æ‰€æœ‰é…ç½®éƒ½åœ¨ `config.py` ä¸­ï¼Œæ— éœ€æ‰‹åŠ¨ä¼ å‚ï¼š

### æ¨¡å‹é…ç½®
```python
# BERTæ¨¡å‹æ¶æ„
vocab_size: int = 30522          # è¯æ±‡è¡¨å¤§å°
hidden_size: int = 768           # éšè—å±‚ç»´åº¦
num_hidden_layers: int = 12      # Transformerå±‚æ•°
num_attention_heads: int = 12    # æ³¨æ„åŠ›å¤´æ•°
max_position_embeddings: int = 512  # æœ€å¤§ä½ç½®åµŒå…¥
```

### è®­ç»ƒé…ç½®
```python
# è®­ç»ƒå‚æ•°
batch_size: int = 16             # æ‰¹æ¬¡å¤§å°
learning_rate: float = 1e-4      # å­¦ä¹ ç‡
num_epochs: int = 3              # è®­ç»ƒè½®æ•°
max_samples: int = 1000          # æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
```

### æ•°æ®é…ç½®
```python
# æ•°æ®å¤„ç†
tokenizer_name: str = "bert-base-uncased"  # tokenizer
max_length: int = 128            # æœ€å¤§åºåˆ—é•¿åº¦
mlm_probability: float = 0.15    # MLMæ©ç æ¦‚ç‡
```

## ğŸ“Š æ•°æ®æµè½¬è¯¦è§£

### 1. MLMæ©ç é€»è¾‘
```python
# åŸå§‹æ–‡æœ¬: "I love cats"
# Tokenized: [CLS] I love cats [SEP]
# æ©ç å:   [CLS] I [MASK] cats [SEP]
# æ ‡ç­¾:     [-100, -100, love, -100, -100]

# æ©ç ç­–ç•¥ï¼š
# - 15%çš„tokenè¢«é€‰ä¸­è¿›è¡Œæ©ç 
# - 80%æ›¿æ¢ä¸º[MASK]
# - 10%æ›¿æ¢ä¸ºéšæœºtoken  
# - 10%ä¿æŒä¸å˜
```

### 2. æ³¨æ„åŠ›æ©ç 
```python
# è¾“å…¥: [1, 1, 1, 0, 0]  (1=çœŸå®token, 0=padding)
# æ‰©å±•: (batch_size, n_heads, seq_len, seq_len)
# è½¬æ¢: 1->0(å¯æ³¨æ„), 0->-10000(ä¸å¯æ³¨æ„)
# åœ¨softmaxä¸­-10000å˜æˆæ¥è¿‘0çš„æ¦‚ç‡
```

### 3. æ•°æ®å½¢çŠ¶å˜åŒ–
```python
# è¾“å…¥å¤„ç†æµç¨‹ï¼š
input_ids: (batch_size, seq_len)
-> embeddings: (batch_size, seq_len, hidden_size)
-> encoder: (batch_size, seq_len, hidden_size)
-> pooler: (batch_size, hidden_size)
-> ä»»åŠ¡å¤´: (batch_size, vocab_size/num_labels)
```

## ğŸ”§ æ ¸å¿ƒç»„ä»¶è¯´æ˜

### Transformerç»„ä»¶ (`transformer.py`)
- **MultiHeadSelfAttention**: å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
- **FeedForward**: ä½ç½®å‰é¦ˆç½‘ç»œ
- **LayerNorm**: å±‚å½’ä¸€åŒ–
- **AddNorm**: æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
- **TransformerEncoderLayer**: å®Œæ•´çš„ç¼–ç å™¨å±‚

### BERTæ¨¡å‹ (`model.py`)
- **BertEmbeddings**: è¯åµŒå…¥ + ä½ç½®åµŒå…¥ + ç±»å‹åµŒå…¥
- **BertEncoder**: å¤šå±‚Transformerç¼–ç å™¨
- **BertPooler**: åºåˆ—æ± åŒ–å±‚
- **BertForPreTraining**: é¢„è®­ç»ƒæ¨¡å‹ï¼ˆMLM + NSPï¼‰
- **BertForSequenceClassification**: åˆ†ç±»æ¨¡å‹

### æ•°æ®å¤„ç† (`data_loader.py`)
- **BertDataCollator**: åŠ¨æ€MLMæ©ç 
- **BertPretrainingDataset**: é¢„è®­ç»ƒæ•°æ®é›†ï¼ˆæ”¯æŒNSPï¼‰
- **BertClassificationDataset**: åˆ†ç±»æ•°æ®é›†

## ğŸ“ˆ è®­ç»ƒç›‘æ§

### é¢„è®­ç»ƒæŒ‡æ ‡
- **æ€»æŸå¤±**: MLMæŸå¤± + NSPæŸå¤±
- **MLMæŸå¤±**: æ©ç è¯­è¨€æ¨¡å‹æŸå¤±
- **NSPæŸå¤±**: ä¸‹ä¸€å¥é¢„æµ‹æŸå¤±
- **å­¦ä¹ ç‡**: åŠ¨æ€å­¦ä¹ ç‡å˜åŒ–

### å¾®è°ƒæŒ‡æ ‡
- **è®­ç»ƒæŸå¤±**: åˆ†ç±»æŸå¤±
- **éªŒè¯å‡†ç¡®ç‡**: åˆ†ç±»å‡†ç¡®ç‡
- **F1åˆ†æ•°**: åŠ æƒF1åˆ†æ•°
- **ç²¾ç¡®ç‡/å¬å›ç‡**: è¯¦ç»†åˆ†ç±»æŒ‡æ ‡

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### æ©ç é¢„æµ‹
```python
from inference import BertInference

inference = BertInference("./bert2_output/best_model", "pretraining")
results = inference.predict_masked_tokens("The capital of France is [MASK].")
# è¾“å‡º: Paris, Lyon, Marseille...
```

### æ–‡æœ¬åˆ†ç±»
```python
inference = BertInference("./bert2_output_finetune/best_model", "classification")
result = inference.classify_text("This movie is amazing!")
# è¾“å‡º: {"predicted_class": 1, "confidence": 0.95}
```

### æ–‡æœ¬ç›¸ä¼¼åº¦
```python
similarity = inference.compute_text_similarity("I love cats", "I adore felines")
# è¾“å‡º: 0.85
```

## ğŸ” å­¦ä¹ è¦ç‚¹

### 1. æ³¨æ„åŠ›æœºåˆ¶
- ç†è§£Qã€Kã€Vçš„è®¡ç®—è¿‡ç¨‹
- æŒæ¡ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›å…¬å¼
- å­¦ä¹ å¤šå¤´æ³¨æ„åŠ›çš„å¹¶è¡Œè®¡ç®—

### 2. æ©ç æœºåˆ¶
- MLMæ©ç çš„éšæœºç­–ç•¥
- æ³¨æ„åŠ›æ©ç çš„åˆ›å»ºå’Œä½¿ç”¨
- paddingæ©ç çš„å¤„ç†

### 3. é¢„è®­ç»ƒä»»åŠ¡
- MLMï¼šå­¦ä¹ åŒå‘è¯­è¨€è¡¨ç¤º
- NSPï¼šå­¦ä¹ å¥å­é—´å…³ç³»

### 4. å¾®è°ƒç­–ç•¥
- æƒé‡åˆå§‹åŒ–å’ŒåŠ è½½
- å­¦ä¹ ç‡è°ƒæ•´
- ä»»åŠ¡ç‰¹å®šå¤´éƒ¨è®¾è®¡

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **å†…å­˜ä½¿ç”¨**: é»˜è®¤é…ç½®éœ€è¦è¾ƒå¤§å†…å­˜ï¼Œå¯ä»¥è°ƒæ•´batch_sizeå’Œmax_samples
2. **è®­ç»ƒæ—¶é—´**: å®Œæ•´è®­ç»ƒéœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œå»ºè®®å…ˆè¿è¡Œquickæµ‹è¯•
3. **æ•°æ®é›†**: ä½¿ç”¨WikiTextå’ŒIMDBæ•°æ®é›†ï¼Œé¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½
4. **è®¾å¤‡æ”¯æŒ**: è‡ªåŠ¨æ£€æµ‹CUDA/MPS/CPUï¼Œä¼˜å…ˆä½¿ç”¨GPU

## ğŸ¤ æ‰©å±•å»ºè®®

1. **æ·»åŠ æ›´å¤šä»»åŠ¡**: å®ç°é—®ç­”ã€å‘½åå®ä½“è¯†åˆ«ç­‰ä»»åŠ¡
2. **ä¼˜åŒ–è®­ç»ƒ**: æ·»åŠ æ··åˆç²¾åº¦è®­ç»ƒã€æ¢¯åº¦ç´¯ç§¯ç­‰
3. **æ¨¡å‹å‹ç¼©**: å®ç°çŸ¥è¯†è’¸é¦ã€å‰ªæç­‰æŠ€æœ¯
4. **å¯è§†åŒ–**: æ·»åŠ æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–åŠŸèƒ½

## ğŸ“š å‚è€ƒèµ„æ–™

- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers/)

---

è¿™ä¸ªæ¡†æ¶ä¸“ä¸ºå­¦ä¹ è®¾è®¡ï¼Œä»£ç ç»“æ„æ¸…æ™°ï¼Œæ³¨é‡Šè¯¦ç»†ã€‚é€šè¿‡é˜…è¯»å’Œè¿è¡Œä»£ç ï¼Œä½ å¯ä»¥æ·±å…¥ç†è§£BERTçš„å·¥ä½œåŸç†å’Œå®ç°ç»†èŠ‚ã€‚


æˆ‘å‡†å¤‡è¿™æ ·å®‰æ’transformeræ¨¡å‹å­˜æ”¾çš„ç›®å½•
é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¨¡å‹ä¿å­˜ç›®å½• saved_model/transformer/pretrain/checkpoint
é¢„è®­ç»ƒæœ€ä½³æ¨¡å‹ä¿å­˜ç›®å½• saved_model/transformer/pretrain/best
é¢„è®­ç»ƒå®Œæˆåæœ€ç»ˆæ¨¡å‹ä¿å­˜ç›®å½• saved_model/transformer/pretrain/final

å¾®è°ƒè¿‡ç¨‹ä¸­çš„æ¨¡å‹ä¿å­˜ç›®å½• saved_model/bert/finetuning/checkpoint
å¾®è°ƒè¿‡ç¨‹ä¸­æœ€ä½³æ¨¡å‹ä¿å­˜ç›®å½• saved_model/bert/finetuning/best
å¾®è°ƒå®Œæˆåæœ€ç»ˆæ¨¡å‹ä¿å­˜ç›®å½• saved_model/bert/finetuning/final

æ‰€æœ‰ç›®å½•ä¼šåœ¨é¦–æ¬¡ä½¿ç”¨æ—¶è‡ªåŠ¨åˆ›å»ºï¼Œè¯·æŒ‰ä¸Šè¿°ç›®å½•å®‰æ’æ”¹æ­£ä»£ç ï¼Œè¦æ±‚è¿™äº›ç›®å½•åœ¨config.pyä¸­ç»Ÿä¸€ç®¡ç†


