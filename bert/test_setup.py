"""
æµ‹è¯•è„šæœ¬ - éªŒè¯BERT2æ¡†æ¶çš„å®‰è£…å’ŒåŸºæœ¬åŠŸèƒ½
"""

import torch
import sys
from pathlib import Path


def test_imports():
    """æµ‹è¯•æ‰€æœ‰æ¨¡å—å¯¼å…¥"""
    print("ğŸ” æµ‹è¯•æ¨¡å—å¯¼å…¥...")

    try:
        from config import (
            BERT_CONFIG,
            TRAINING_CONFIG,
            DATA_CONFIG,
            get_device,
            print_config,
        )

        print("âœ… configæ¨¡å—å¯¼å…¥æˆåŠŸ")

        from transformer import MultiHeadSelfAttention, TransformerEncoderLayer

        print("âœ… transformeræ¨¡å—å¯¼å…¥æˆåŠŸ")

        from model import BertModel, BertForPreTraining, BertForSequenceClassification

        print("âœ… modelæ¨¡å—å¯¼å…¥æˆåŠŸ")

        from data_loader import BertDataCollator, create_pretraining_dataloader

        print("âœ… data_loaderæ¨¡å—å¯¼å…¥æˆåŠŸ")

        from trainer import BertTrainer

        print("âœ… traineræ¨¡å—å¯¼å…¥æˆåŠŸ")

        from fine_tuning import BertFineTuner

        print("âœ… fine_tuningæ¨¡å—å¯¼å…¥æˆåŠŸ")

        from inference import BertInference

        print("âœ… inferenceæ¨¡å—å¯¼å…¥æˆåŠŸ")

        return True

    except Exception as e:
        print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False


def test_device():
    """æµ‹è¯•è®¾å¤‡æ£€æµ‹"""
    print("\nğŸ–¥ï¸ æµ‹è¯•è®¾å¤‡æ£€æµ‹...")

    try:
        from config import get_device

        device = get_device()
        print(f"âœ… æ£€æµ‹åˆ°è®¾å¤‡: {device}")

        # æµ‹è¯•tensoråˆ›å»º
        x = torch.randn(2, 3).to(device)
        print(f"âœ… åœ¨{device}ä¸Šåˆ›å»ºtensoræˆåŠŸ: {x.shape}")

        return True

    except Exception as e:
        print(f"âŒ è®¾å¤‡æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_model_creation():
    """æµ‹è¯•æ¨¡å‹åˆ›å»º"""
    print("\nğŸ—ï¸ æµ‹è¯•æ¨¡å‹åˆ›å»º...")

    try:
        from model import BertForPreTraining, BertForSequenceClassification
        from config import get_device

        device = get_device()

        # æµ‹è¯•é¢„è®­ç»ƒæ¨¡å‹
        print("åˆ›å»ºé¢„è®­ç»ƒæ¨¡å‹...")
        pretrain_model = BertForPreTraining()
        pretrain_model.to(device)
        print(f"âœ… é¢„è®­ç»ƒæ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {pretrain_model.count_parameters():,}")

        # æµ‹è¯•åˆ†ç±»æ¨¡å‹
        print("åˆ›å»ºåˆ†ç±»æ¨¡å‹...")
        classification_model = BertForSequenceClassification(num_labels=2)
        classification_model.to(device)
        print(f"âœ… åˆ†ç±»æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {classification_model.count_parameters():,}")

        return True

    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return False


def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½"""
    print("\nğŸ“Š æµ‹è¯•æ•°æ®åŠ è½½...")

    try:
        from data_loader import BertDataCollator, BertPretrainingDataset
        from transformers import AutoTokenizer
        from config import DATA_CONFIG

        # åŠ è½½tokenizer
        print("åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(DATA_CONFIG.tokenizer_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        if tokenizer.mask_token is None:
            tokenizer.add_special_tokens({"mask_token": "[MASK]"})
        print("âœ… tokenizeråŠ è½½æˆåŠŸ")

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        print("åˆ›å»ºæµ‹è¯•æ•°æ®é›†...")
        test_texts = [
            "The quick brown fox jumps over the lazy dog.",
            "Machine learning is a subset of artificial intelligence.",
            "Natural language processing enables computers to understand human language.",
        ]

        dataset = BertPretrainingDataset(test_texts, tokenizer)
        print(f"âœ… æ•°æ®é›†åˆ›å»ºæˆåŠŸï¼Œæ ·æœ¬æ•°é‡: {len(dataset)}")

        # æµ‹è¯•æ•°æ®æ•´ç†å™¨
        print("æµ‹è¯•æ•°æ®æ•´ç†å™¨...")
        collator = BertDataCollator(tokenizer)

        # è·å–ä¸€ä¸ªæ‰¹æ¬¡
        batch_samples = [dataset[i] for i in range(min(2, len(dataset)))]
        batch = collator(batch_samples)

        print(f"âœ… æ‰¹æ¬¡æ•°æ®åˆ›å»ºæˆåŠŸ:")
        for key, value in batch.items():
            if isinstance(value, torch.Tensor):
                print(f"  {key}: {value.shape}")
            else:
                print(f"  {key}: {type(value)}")

        return True

    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_forward_pass():
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    print("\nâš¡ æµ‹è¯•å‰å‘ä¼ æ’­...")

    try:
        from model import BertForPreTraining
        from data_loader import BertDataCollator, BertPretrainingDataset
        from transformers import AutoTokenizer
        from config import DATA_CONFIG, get_device

        device = get_device()

        # åˆ›å»ºæ¨¡å‹
        model = BertForPreTraining()
        model.to(device)
        model.eval()

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        tokenizer = AutoTokenizer.from_pretrained(DATA_CONFIG.tokenizer_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        if tokenizer.mask_token is None:
            tokenizer.add_special_tokens({"mask_token": "[MASK]"})

        test_texts = ["Hello world. This is a test."]
        dataset = BertPretrainingDataset(test_texts, tokenizer)
        collator = BertDataCollator(tokenizer)

        # åˆ›å»ºæ‰¹æ¬¡
        batch_samples = [dataset[0]]
        batch = collator(batch_samples)
        batch = {k: v.to(device) for k, v in batch.items()}

        # å‰å‘ä¼ æ’­
        print("æ‰§è¡Œå‰å‘ä¼ æ’­...")
        with torch.no_grad():
            outputs = model(**batch)

        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ:")
        for key, value in outputs.items():
            if isinstance(value, torch.Tensor):
                print(f"  {key}: {value.shape}")
            elif value is not None:
                print(f"  {key}: {type(value)}")

        return True

    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_config():
    """æµ‹è¯•é…ç½®"""
    print("\nâš™ï¸ æµ‹è¯•é…ç½®...")

    try:
        from config import print_config, BERT_CONFIG, TRAINING_CONFIG, DATA_CONFIG

        print("é…ç½®ä¿¡æ¯:")
        print_config()

        # éªŒè¯é…ç½®çš„åˆç†æ€§
        assert BERT_CONFIG.hidden_size % BERT_CONFIG.num_attention_heads == 0, "hidden_sizeå¿…é¡»èƒ½è¢«num_attention_headsæ•´é™¤"
        assert TRAINING_CONFIG.batch_size > 0, "batch_sizeå¿…é¡»å¤§äº0"
        assert DATA_CONFIG.max_length > 0, "max_lengthå¿…é¡»å¤§äº0"

        print("âœ… é…ç½®éªŒè¯é€šè¿‡")

        return True

    except Exception as e:
        print(f"âŒ é…ç½®æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª BERT2æ¡†æ¶æµ‹è¯•")
    print("=" * 50)

    tests = [
        ("æ¨¡å—å¯¼å…¥", test_imports),
        ("è®¾å¤‡æ£€æµ‹", test_device),
        ("é…ç½®éªŒè¯", test_config),
        ("æ¨¡å‹åˆ›å»º", test_model_creation),
        ("æ•°æ®åŠ è½½", test_data_loading),
        ("å‰å‘ä¼ æ’­", test_forward_pass),
    ]

    passed = 0
    total = len(tests)

    for test_name, test_func in tests:
        try:
            if test_func():
                passed += 1
            else:
                print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")

    print("\n" + "=" * 50)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")

    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¡†æ¶å®‰è£…æ­£ç¡®ã€‚")
        print("\nğŸ“š æ¥ä¸‹æ¥ä½ å¯ä»¥:")
        print("  1. è¿è¡Œå¿«é€Ÿæµ‹è¯•: python main.py quick")
        print("  2. æŸ¥çœ‹é…ç½®: python -c 'from config import print_config; print_config()'")
        print("  3. å¼€å§‹é¢„è®­ç»ƒ: python main.py pretrain")
        return True
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®‰è£…å’Œé…ç½®ã€‚")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
